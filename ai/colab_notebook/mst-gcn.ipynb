{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91f1b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 0. 기본 라이브러리 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import pickle\n",
    "from time import time\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c28d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 1. 유틸리티 함수 정의\n",
    "\n",
    "def scaled_Laplacian(W):\n",
    "    '''\n",
    "    compute \\tilde{L}\n",
    "    '''\n",
    "    assert W.shape[0] == W.shape[1]\n",
    "    D = np.diag(np.sum(W, axis=1))\n",
    "    L = D - W\n",
    "    lambda_max = eigs(L, k=1, which='LR')[0].real\n",
    "    return (2 * L) / lambda_max - np.identity(W.shape[0])\n",
    "\n",
    "def cheb_polynomial(L_tilde, K):\n",
    "    '''\n",
    "    compute a list of chebyshev polynomials from T_0 to T_{K-1}\n",
    "    '''\n",
    "    N = L_tilde.shape[0]\n",
    "    cheb_polynomials = [np.identity(N), L_tilde.copy()]\n",
    "    for i in range(2, K):\n",
    "        cheb_polynomials.append(2 * L_tilde @ cheb_polynomials[i - 1] - cheb_polynomials[i - 2])\n",
    "    return cheb_polynomials\n",
    "\n",
    "def create_multi_scale_sequences(data, hour_len=6, day_len=24, week_len=24, \n",
    "                                output_len=24, week_offset=168):\n",
    "    \"\"\"\n",
    "    Create multi-scale input sequences for MST-GCN\n",
    "    Updated for 1-hour timesteps\n",
    "    \"\"\"\n",
    "    X_hour, X_day, X_week, y = [], [], [], []\n",
    "    \n",
    "    # Start from week_offset to ensure we have enough history\n",
    "    for i in range(week_offset, data.shape[2] - output_len + 1):\n",
    "        # Recent pattern (last 6 hours)\n",
    "        hour_slice = data[:, :, i-hour_len:i]\n",
    "        X_hour.append(hour_slice)\n",
    "        \n",
    "        # Daily pattern (last 24 hours)\n",
    "        day_slice = data[:, :, i-day_len:i]\n",
    "        X_day.append(day_slice)\n",
    "        \n",
    "        # Weekly pattern (same 24 hours from last week)\n",
    "        week_slice = data[:, :, i-week_offset:i-week_offset+week_len]\n",
    "        X_week.append(week_slice)\n",
    "        \n",
    "        # Target (next 24 hours, DRT_Demand_Prob which is index 1)\n",
    "        y_slice = data[1, :, i:i+output_len]  # drt_probability is index 1\n",
    "        y.append(y_slice)\n",
    "    \n",
    "    return np.array(X_hour), np.array(X_day), np.array(X_week), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d06cb9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 2. MST-GCN 모델 아키텍처\n",
    "\n",
    "class cheb_conv(nn.Module):\n",
    "    '''\n",
    "    K-order chebyshev graph convolution\n",
    "    '''\n",
    "    def __init__(self, K, cheb_polynomials, in_channels, out_channels):\n",
    "        super(cheb_conv, self).__init__()\n",
    "        self.K = K\n",
    "        self.cheb_polynomials = cheb_polynomials\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.DEVICE = cheb_polynomials[0].device\n",
    "        \n",
    "        self.Theta = nn.ParameterList([\n",
    "            nn.Parameter(torch.FloatTensor(in_channels, out_channels).to(self.DEVICE)) \n",
    "            for _ in range(K)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_of_vertices, in_channels, num_of_timesteps = x.shape\n",
    "        \n",
    "        outputs = []\n",
    "        for time_step in range(num_of_timesteps):\n",
    "            graph_signal = x[:, :, :, time_step]\n",
    "            output = torch.zeros(batch_size, num_of_vertices, self.out_channels).to(self.DEVICE)\n",
    "            \n",
    "            for k in range(self.K):\n",
    "                T_k = self.cheb_polynomials[k]\n",
    "                theta_k = self.Theta[k]\n",
    "                rhs = torch.matmul(T_k, graph_signal)\n",
    "                output = output + torch.matmul(rhs, theta_k)\n",
    "                \n",
    "            outputs.append(output.unsqueeze(-1))\n",
    "            \n",
    "        return F.relu(torch.cat(outputs, dim=-1))\n",
    "\n",
    "\n",
    "class MSTGCN_block(nn.Module):\n",
    "    def __init__(self, DEVICE, in_channels, K, nb_chev_filter, nb_time_filter, \n",
    "                 time_conv_strides, cheb_polynomials, num_of_vertices, num_of_timesteps):\n",
    "        super(MSTGCN_block, self).__init__()\n",
    "        \n",
    "        self.cheb_conv = cheb_conv(K, cheb_polynomials, in_channels, nb_chev_filter)\n",
    "        self.time_conv = nn.Conv2d(nb_chev_filter, nb_time_filter, \n",
    "                                   kernel_size=(1, 3), stride=(1, time_conv_strides), \n",
    "                                   padding=(0, 1))\n",
    "        self.residual_conv = nn.Conv2d(in_channels, nb_time_filter, \n",
    "                                       kernel_size=(1, 1), stride=(1, time_conv_strides))\n",
    "        self.ln = nn.LayerNorm(nb_time_filter)\n",
    "\n",
    "    def forward(self, x):\n",
    "        spatial_gcn = self.cheb_conv(x)\n",
    "        time_conv_output = self.time_conv(spatial_gcn.permute(0, 2, 1, 3))\n",
    "        time_conv_output = time_conv_output.permute(0, 2, 1, 3)\n",
    "        \n",
    "        x_residual = self.residual_conv(x.permute(0, 2, 1, 3))\n",
    "        x_residual = x_residual.permute(0, 2, 1, 3)\n",
    "        \n",
    "        out = F.relu(x_residual + time_conv_output)\n",
    "        out = out.permute(0, 1, 3, 2)\n",
    "        out = self.ln(out)\n",
    "        out = out.permute(0, 1, 3, 2)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class MSTGCN_submodule(nn.Module):\n",
    "    def __init__(self, DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter, \n",
    "                 time_strides, cheb_polynomials, num_for_predict, len_input, num_of_vertices):\n",
    "        super(MSTGCN_submodule, self).__init__()\n",
    "        \n",
    "        self.BlockList = nn.ModuleList()\n",
    "        \n",
    "        self.BlockList.append(\n",
    "            MSTGCN_block(DEVICE, in_channels, K, nb_chev_filter, nb_time_filter, \n",
    "                         time_strides, cheb_polynomials, num_of_vertices, len_input)\n",
    "        )\n",
    "        \n",
    "        for _ in range(nb_block - 1):\n",
    "            self.BlockList.append(\n",
    "                MSTGCN_block(DEVICE, nb_time_filter, K, nb_chev_filter, nb_time_filter, \n",
    "                             1, cheb_polynomials, num_of_vertices, len_input // time_strides)\n",
    "            )\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(int(len_input / time_strides), num_for_predict, \n",
    "                                   kernel_size=(1, nb_time_filter))\n",
    "        self.W = nn.Parameter(torch.FloatTensor(num_of_vertices, num_for_predict))\n",
    "        \n",
    "        self.DEVICE = DEVICE\n",
    "        self.to(DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.BlockList:\n",
    "            x = block(x)\n",
    "            \n",
    "        output = self.final_conv(x.permute(0, 3, 1, 2))\n",
    "        output = output[:, :, :, 0].permute(0, 2, 1)\n",
    "        output = output * self.W\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class MSTGCN(nn.Module):\n",
    "    '''\n",
    "    Multi-Scale Temporal Graph Convolutional Networks\n",
    "    '''\n",
    "    def __init__(self, DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter,\n",
    "                 time_strides, cheb_polynomials, num_for_predict, num_of_vertices,\n",
    "                 len_hour, len_day, len_week):\n",
    "        super(MSTGCN, self).__init__()\n",
    "        \n",
    "        self.num_for_predict = num_for_predict\n",
    "        \n",
    "        self.hour_module = MSTGCN_submodule(\n",
    "            DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter,\n",
    "            time_strides, cheb_polynomials, num_for_predict, len_hour, num_of_vertices\n",
    "        )\n",
    "        \n",
    "        self.day_module = MSTGCN_submodule(\n",
    "            DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter,\n",
    "            time_strides, cheb_polynomials, num_for_predict, len_day, num_of_vertices\n",
    "        )\n",
    "        \n",
    "        self.week_module = MSTGCN_submodule(\n",
    "            DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter,\n",
    "            time_strides, cheb_polynomials, num_for_predict, len_week, num_of_vertices\n",
    "        )\n",
    "\n",
    "    def forward(self, x_hour, x_day, x_week):\n",
    "        hour_output = self.hour_module(x_hour)\n",
    "        day_output = self.day_module(x_day)\n",
    "        week_output = self.week_module(x_week)\n",
    "        \n",
    "        output = hour_output + day_output + week_output\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e17d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "#@title 3. 데이터 로딩 및 전처리 (8개월 통합 데이터)\n\n# Google Colab에서 학습할 때 사용할 코드\n# 로컬에서 8개월 데이터를 통합한 dataset.npz 파일 사용\n\n# 🔄 수정: 통합된 8개월 데이터 로딩\nbase_path = '/content/drive/MyDrive/train_dataset/dataset.npz'\ndata = np.load(base_path)\n\nfeature_matrix = data['feature_matrix']  # (5, N, T) - 4개 입력 + 1개 타겟\nstop_ids = data['stop_ids'] \nadj_mx = data['adj_matrix']\n\nprint(\"✅ 통합된 8개월 MST-GCN 데이터 로딩 완료:\")\nprint(f\"Feature matrix shape: {feature_matrix.shape}\")\nprint(f\"정류장 수: {len(stop_ids)}\")\nprint(f\"인접 행렬 shape: {adj_mx.shape}\")\n\n# 피처 구성 확인\nprint(\"\\n피처 구성 (5개):\")\nprint(\"  [0] normalized_log_boarding_count - Log+Z-score 정규화 수요\")\nprint(\"  [1] service_availability - 서비스 가용성 (0,1,2)\")\nprint(\"  [2] is_rest_day - 휴식일 여부 (0,1)\")  \nprint(\"  [3] normalized_interval - 정규화된 배차간격 [0,1]\")\nprint(\"  [4] drt_probability - DRT 확률 (타겟)\")\n\n# 입력 피처와 타겟 분리\ninput_features = feature_matrix[:4, :, :]  # 처음 4개가 입력\ntarget_feature = feature_matrix[4, :, :]   # 마지막이 타겟\n\nprint(f\"\\n입력 피처 shape: {input_features.shape}\")\nprint(f\"타겟 피처 shape: {target_feature.shape}\")\n\n# 인접 행렬 통계\nedge_count = np.sum(adj_mx) / 2\navg_degree = np.sum(adj_mx, axis=1).mean()\nprint(f\"\\n인접 행렬 통계:\")\nprint(f\"총 엣지 수: {edge_count}\")\nprint(f\"평균 차수: {avg_degree:.2f}\")\n\n# 피처별 통계 출력\nfor i, feature_name in enumerate(['norm_log_boarding', 'service_avail', 'is_rest_day', 'norm_interval']):\n    feat_data = input_features[i]\n    print(f\"\\n{feature_name} 통계:\")\n    print(f\"  범위: [{feat_data.min():.4f}, {feat_data.max():.4f}]\")\n    print(f\"  평균: {feat_data.mean():.4f}\")\n    print(f\"  표준편차: {feat_data.std():.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b0bbfa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "#@title 4. 학습 시퀀스 로딩 (통합된 8개월 데이터에서 직접)\n\n# 데이터 로더(data_preparation/mstgcn_data_loader.py)가\n# 이미 생성한 다중 스케일 시퀀스를 통합된 npz 파일에서 직접 로드합니다.\n# 이 셀은 기존의 시퀀스 생성 코드를 대체합니다.\n\n# Cell 3에서 로드한 'data' 객체에서 각 배열을 가져옵니다.\nX_hour = data['X_hour']\nX_day = data['X_day']\nX_week = data['X_week']\ny = data['y']\n\nprint(\"✅ 사전 처리된 다중 스케일 시퀀스를 통합된 8개월 데이터에서 직접 로드했습니다.\")\nprint(\"\\n--- 로드된 데이터 Shape ---\")\nprint(f\"X_hour: {X_hour.shape}\")\nprint(f\"X_day:  {X_day.shape}\")\nprint(f\"X_week: {X_week.shape}\")\nprint(f\"y:      {y.shape}\")\n\nprint(f\"\\n📊 8개월 통합 데이터 통계:\")\nprint(f\"총 샘플 수: {X_hour.shape[0]}\")\nprint(f\"정류장 수: {X_hour.shape[1]}\")\nprint(f\"입력 피처 수: {X_hour.shape[2]}\")\nprint(f\"예측 길이: {y.shape[2]} 시간\")\n\n# 다음 셀에서 사용할 변수들이 올바르게 로드되었는지 확인\nassert 'adj_mx' in locals() or 'adj_mx' in globals(), \"오류: 'adj_mx'가 이전 셀에서 로드되지 않았습니다.\"\nassert X_hour.shape[2] == 4, f\"오류: 입력 피처의 수가 4가 아닙니다. 감지된 수: {X_hour.shape[2]}\"\n\nprint(\"\\n✅ 데이터 구조 검증 완료. 다음 단계 진행 가능.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414beaf9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "#@title 5. 하이퍼파라미터 및 설정 정의 (4개 입력 피처)\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', DEVICE)\n\n# 학습 하이퍼파라미터\nLEARNING_RATE = 0.001\nEPOCHS = 100\nBATCH_SIZE = 32\n\n# MST-GCN 모델 파라미터\nK = 3                  # Chebyshev 차수 : 그래프 컨볼루션 연산 시, 각 노드가 몇 홉(hop) 떨어진 이웃의 정보까지 참고할 것인지를 의미\nnb_block = 2           # 블록 수 : 하나의 서브모듈(hour, day, week) 안에 Spatio-Temporal 블록을 몇 개나 쌓을 것인지\nnb_chev_filter = 64    # 공간 필터 수 : 그래프 컨볼루션(공간)을 통해 추출할 특징(피처)의 개수\nnb_time_filter = 64    # 시간 필터 수 : 시간적 컨볼루션을 통해 추출할 특징(피처)의 개수\ntime_strides = 1       # 스트라이드 : 시간적 컨볼루션 연산 시, 몇 칸씩 건너뛰며 적용할지 결정(1인 경우 모든 시점 체크)\n\n# 데이터 shape 기반 파라미터(shape에 따라 자동 결정)\nnum_of_vertices = adj_mx.shape[0]\nin_channels = X_hour.shape[2]  # 4개 입력 피처\nnum_for_predict = y.shape[2]   # 24시간 예측\nlen_hour = X_hour.shape[3]     # 6시간\nlen_day = X_day.shape[3]       # 24시간\nlen_week = X_week.shape[3]     # 24시간\n\nprint(f\"\\n모델 파라미터 (4개 입력 피처 기준):\")\nprint(f\"정류장 수: {num_of_vertices}\")\nprint(f\"입력 피처 수: {in_channels} (normalized_log_boarding_count, service_availability, is_rest_day, normalized_interval)\")\nprint(f\"예측 시간 길이: {num_for_predict}\")\nprint(f\"Hour 스케일 길이: {len_hour}\")\nprint(f\"Day 스케일 길이: {len_day}\")\nprint(f\"Week 스케일 길이: {len_week}\")\n\n# 모델 입력 검증\nassert in_channels == 4, f\"입력 피처 수가 4개가 아닙니다: {in_channels}\"\nprint(f\"\\n✅ 4개 입력 피처 확인 완료\")\n\n# 각 피처의 의미 설명\nprint(f\"\\n입력 피처 구성:\")\nprint(f\"  피처 0: normalized_log_boarding_count - Log+Z-score 정규화된 수요 (연속값)\")\nprint(f\"  피처 1: service_availability - 서비스 가용성 (0=비운행, 1=시간외, 2=시간내)\")\nprint(f\"  피처 2: is_rest_day - 휴식일 여부 (0=평일, 1=휴일)\")\nprint(f\"  피처 3: normalized_interval - 정규화된 배차간격 (0~1)\")\n\n# 데이터 범위 검증\nprint(f\"\\n데이터 범위 검증:\")\nfor i, feature_name in enumerate(['norm_log_boarding', 'service_avail', 'is_rest_day', 'norm_interval']):\n    feat_data = X_hour[:, :, i, :]  # 각 피처의 데이터\n    print(f\"  {feature_name}: [{feat_data.min():.4f}, {feat_data.max():.4f}]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914f3da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "#@title 6. 데이터 로더 생성 (🚨 Data Leakage 방지 - 시간 순서 기반 분할)\n\n# numpy 배열을 torch 텐서로 변환\nX_hour_tensor = torch.from_numpy(X_hour).type(torch.FloatTensor)\nX_day_tensor = torch.from_numpy(X_day).type(torch.FloatTensor)\nX_week_tensor = torch.from_numpy(X_week).type(torch.FloatTensor)\ny_tensor = torch.from_numpy(y).type(torch.FloatTensor)\n\n# Custom Dataset for multi-scale inputs\nclass MultiScaleDataset(torch.utils.data.Dataset):\n    def __init__(self, X_hour, X_day, X_week, y):\n        self.X_hour = X_hour\n        self.X_day = X_day\n        self.X_week = X_week\n        self.y = y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return self.X_hour[idx], self.X_day[idx], self.X_week[idx], self.y[idx]\n\n# 전체 데이터셋 생성\ndataset = MultiScaleDataset(X_hour_tensor, X_day_tensor, X_week_tensor, y_tensor)\n\n# 🚨 중요: 시간 순서 기반 분할 (Data Leakage 방지)\n# random_split 대신 시간 순서를 보장하는 분할 사용\ndataset_size = len(dataset)\ntrain_end = int(dataset_size * 0.7)\nval_end = int(dataset_size * 0.85)\n\n# 시간 순서대로 인덱스 생성\ntrain_indices = list(range(0, train_end))\nval_indices = list(range(train_end, val_end))\ntest_indices = list(range(val_end, dataset_size))\n\nprint(f\"🚨 Data Leakage 방지를 위한 시간 순서 기반 분할:\")\nprint(f\"   Train: 샘플 [0:{train_end}] → 초기 70% 시간대 ({len(train_indices)} 샘플)\")\nprint(f\"   Val:   샘플 [{train_end}:{val_end}] → 중간 15% 시간대 ({len(val_indices)} 샘플)\")\nprint(f\"   Test:  샘플 [{val_end}:{dataset_size}] → 마지막 15% 시간대 ({len(test_indices)} 샘플)\")\n\n# Subset으로 데이터셋 분할\nfrom torch.utils.data import Subset\ntrain_dataset = Subset(dataset, train_indices)\nval_dataset = Subset(dataset, val_indices)\ntest_dataset = Subset(dataset, test_indices)\n\n# DataLoader 생성 (Train만 셔플, Val/Test는 시간 순서 유지)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f\"\\n📊 최종 데이터셋 크기:\")\nprint(f\"전체: {len(dataset)} 샘플 (8개월 통합)\")\nprint(f\"학습: {len(train_dataset)} 샘플\")\nprint(f\"검증: {len(val_dataset)} 샘플\")\nprint(f\"테스트: {len(test_dataset)} 샘플\")\n\nprint(f\"\\n✅ 시계열 특성을 보존한 신뢰할 수 있는 데이터 분할 완료\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c363732",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 7. 모델, 손실 함수, 옵티마이저 초기화\n",
    "\n",
    "# 체비셰프 다항식 계산\n",
    "L_tilde = scaled_Laplacian(adj_mx)\n",
    "cheb_polynomials = [torch.from_numpy(i).type(torch.FloatTensor).to(DEVICE) for i in cheb_polynomial(L_tilde, K)]\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = MSTGCN(\n",
    "    DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter,\n",
    "    time_strides, cheb_polynomials, num_for_predict, num_of_vertices,\n",
    "    len_hour, len_day, len_week\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# 손실 함수: MSE (DRT 확률 예측용)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 옵티마이저\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 모델 가중치 초기화\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    else:\n",
    "        nn.init.uniform_(p)\n",
    "\n",
    "print(\"MST-GCN 모델 구조:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a6724",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 8. 모델 학습 및 검증 루프\n",
    "\n",
    "print(\"\\n학습 시작...\")\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = -1\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "# 학습 데이터 통계 (정규화용)\n",
    "train_indices = train_dataset.indices\n",
    "X_hour_train = X_hour_tensor[train_indices]\n",
    "X_day_train = X_day_tensor[train_indices]\n",
    "X_week_train = X_week_tensor[train_indices]\n",
    "\n",
    "# 각 스케일별 평균과 표준편차 계산\n",
    "hour_mean = X_hour_train.mean()\n",
    "hour_std = X_hour_train.std()\n",
    "day_mean = X_day_train.mean()\n",
    "day_std = X_day_train.std()\n",
    "week_mean = X_week_train.mean()\n",
    "week_std = X_week_train.std()\n",
    "\n",
    "print(f\"정규화 통계:\")\n",
    "print(f\"Hour - Mean: {hour_mean:.4f}, Std: {hour_std:.4f}\")\n",
    "print(f\"Day - Mean: {day_mean:.4f}, Std: {day_std:.4f}\")\n",
    "print(f\"Week - Mean: {week_mean:.4f}, Std: {week_std:.4f}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 학습 단계\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for x_hour, x_day, x_week, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "        x_hour, x_day, x_week, y_batch = x_hour.to(DEVICE), x_day.to(DEVICE), x_week.to(DEVICE), y_batch.to(DEVICE)\n",
    "        \n",
    "        # 순전파\n",
    "        output = model(x_hour, x_day, x_week)\n",
    "        loss = loss_fn(output, y_batch)\n",
    "        \n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    \n",
    "    # 검증 단계\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_hour, x_day, x_week, y_val in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            x_hour, x_day, x_week, y_val = x_hour.to(DEVICE), x_day.to(DEVICE), x_week.to(DEVICE), y_val.to(DEVICE)\n",
    "            output = model(x_hour, x_day, x_week)\n",
    "            loss = loss_fn(output, y_val)\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # 최고 성능 모델 저장\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), 'best_mstgcn_model.pth')\n",
    "        print(f\"-> Best model saved at epoch {epoch+1} with validation loss {best_val_loss:.6f}\")\n",
    "\n",
    "print(\"\\n학습 완료!\")\n",
    "print(f\"Best validation loss {best_val_loss:.6f} at epoch {best_epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b72a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "#@title 9. TorchServe 배포를 위한 모델 및 아티팩트 저장 (8개월 학습 모델)\n\nsave_dir = '/content/drive/MyDrive/MSTGCN_results_8months'\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\n# 1. 전체 모델 저장 (.pt 파일)\nmodel.load_state_dict(torch.load('best_mstgcn_model.pth'))\nmodel.eval()\nfull_model_path = os.path.join(save_dir, 'mstgcn_model_full_8months.pt')\ntorch.save(model, full_model_path)\n\n# 2. 체비셰프 다항식 저장\ncheb_path = os.path.join(save_dir, 'cheb_polynomials.pt')\ncheb_cpu = [poly.cpu() for poly in cheb_polynomials]\ntorch.save(cheb_cpu, cheb_path)\n\n# 3. 인접 행렬 저장\nadj_path = os.path.join(save_dir, 'adjacency_matrix.npy')\nnp.save(adj_path, adj_mx)\n\n# 4. 정규화 통계 저장\nstats_path = os.path.join(save_dir, 'normalization_stats.npz')\nnp.savez(stats_path, \n         hour_mean=hour_mean.numpy(), hour_std=hour_std.numpy(),\n         day_mean=day_mean.numpy(), day_std=day_std.numpy(),\n         week_mean=week_mean.numpy(), week_std=week_std.numpy())\n\n# 5. 정류장 매핑 정보 저장 (8개월 통합 데이터 기준)\nstop_mapping_path = os.path.join(save_dir, 'stop_mapping_8months.pkl')\nstop_mapping = {\n    'stop_ids': stop_ids.tolist(),\n    'num_stops': len(stop_ids),\n    'training_period': '2024-11 to 2025-06 (8 months)',\n    'common_stops_only': True\n}\nwith open(stop_mapping_path, 'wb') as f:\n    pickle.dump(stop_mapping, f)\n\n# 6. 모델 설정 저장 (8개월 데이터 기준)\nconfig_path = os.path.join(save_dir, 'model_config_8months.pkl')\nmodel_config = {\n    'nb_block': nb_block,\n    'in_channels': in_channels,\n    'K': K,\n    'nb_chev_filter': nb_chev_filter,\n    'nb_time_filter': nb_time_filter,\n    'time_strides': time_strides,\n    'num_for_predict': num_for_predict,\n    'num_of_vertices': num_of_vertices,\n    'len_hour': len_hour,\n    'len_day': len_day,\n    'len_week': len_week,\n    'device': str(DEVICE),\n    'training_samples': len(dataset),\n    'training_period': '8 months (2024-11 to 2025-06)',\n    'data_leakage_prevented': True,\n    'temporal_split': True\n}\nwith open(config_path, 'wb') as f:\n    pickle.dump(model_config, f)\n\n# 7. 학습 히스토리 저장\nhistory_path = os.path.join(save_dir, 'training_history_8months.pkl')\nenhanced_history = {\n    'train_loss': history['train_loss'],\n    'val_loss': history['val_loss'],\n    'best_epoch': best_epoch,\n    'best_val_loss': best_val_loss,\n    'total_epochs': EPOCHS,\n    'batch_size': BATCH_SIZE,\n    'learning_rate': LEARNING_RATE,\n    'training_samples': len(train_dataset),\n    'validation_samples': len(val_dataset),\n    'test_samples': len(test_dataset)\n}\nwith open(history_path, 'wb') as f:\n    pickle.dump(enhanced_history, f)\n\nprint(\"\\n✅ TorchServe 배포용 파일 저장 완료 (8개월 학습 모델):\")\nprint(f\"   - 전체 모델: {full_model_path}\")\nprint(f\"   - 체비셰프 다항식: {cheb_path}\")\nprint(f\"   - 인접 행렬: {adj_path}\")\nprint(f\"   - 정규화 통계: {stats_path}\")\nprint(f\"   - 정류장 매핑: {stop_mapping_path}\")\nprint(f\"   - 모델 설정: {config_path}\")\nprint(f\"   - 학습 히스토리: {history_path}\")\nprint(f\"\\n🎯 특징:\")\nprint(f\"   - 8개월 연속 데이터로 학습\")\nprint(f\"   - Data Leakage 방지 적용\")\nprint(f\"   - 시간 순서 기반 분할\")\nprint(f\"   - 총 {len(dataset)} 샘플 학습\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034aa8e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 10. 테스트 셋에서 최종 성능 평가\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_hour, x_day, x_week, y_test in tqdm(test_loader, desc=\"Test Evaluation\"):\n",
    "        x_hour, x_day, x_week, y_test = x_hour.to(DEVICE), x_day.to(DEVICE), x_week.to(DEVICE), y_test.to(DEVICE)\n",
    "        output = model(x_hour, x_day, x_week)\n",
    "        loss = loss_fn(output, y_test)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        predictions.append(output.cpu().numpy())\n",
    "        actuals.append(y_test.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"\\n테스트 셋 MSE Loss: {avg_test_loss:.6f}\")\n",
    "print(f\"테스트 셋 RMSE: {np.sqrt(avg_test_loss):.6f}\")\n",
    "\n",
    "# 예측 결과 저장\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "actuals = np.concatenate(actuals, axis=0)\n",
    "test_results_path = os.path.join(save_dir, 'test_predictions.npz')\n",
    "np.savez(test_results_path, predictions=predictions, actuals=actuals)\n",
    "print(f\"테스트 예측 결과 저장: {test_results_path}\")\n",
    "\n",
    "print(\"\\n🎉 MST-GCN 학습 및 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fc949",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cac7f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 1. (추론) 저장된 모델 및 아티팩트 로드\n",
    "\n",
    "save_dir = '/content/drive/MyDrive/MSTGCN_results'\n",
    "\n",
    "# 1. 전체 모델 로드\n",
    "model_path = os.path.join(save_dir, 'mstgcn_model_full.pt')\n",
    "model = torch.load(model_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval()\n",
    "print(\"✅ 모델 로드 완료\")\n",
    "\n",
    "# 2. 체비셰프 다항식 로드\n",
    "cheb_path = os.path.join(save_dir, 'cheb_polynomials.pt')\n",
    "cheb_polynomials = torch.load(cheb_path)\n",
    "print(\"✅ 체비셰프 다항식 로드 완료\")\n",
    "\n",
    "# 3. 정규화 통계 로드\n",
    "stats_path = os.path.join(save_dir, 'normalization_stats.npz')\n",
    "stats = np.load(stats_path)\n",
    "hour_mean, hour_std = stats['hour_mean'], stats['hour_std']\n",
    "day_mean, day_std = stats['day_mean'], stats['day_std']\n",
    "week_mean, week_std = stats['week_mean'], stats['week_std']\n",
    "print(\"✅ 정규화 통계 로드 완료\")\n",
    "\n",
    "# 4. 정류장 매핑 정보 로드\n",
    "stop_mapping_path = os.path.join(save_dir, 'stop_mapping.pkl')\n",
    "with open(stop_mapping_path, 'rb') as f:\n",
    "    stop_mapping = pickle.load(f)\n",
    "stop_ids = stop_mapping['stop_ids']\n",
    "stop_info = stop_mapping['stop_info']\n",
    "print(f\"✅ 정류장 매핑 로드 완료 ({len(stop_ids)}개 정류장)\")\n",
    "\n",
    "# 5. 모델 설정 로드\n",
    "config_path = os.path.join(save_dir, 'model_config.pkl')\n",
    "with open(config_path, 'rb') as f:\n",
    "    model_config = pickle.load(f)\n",
    "print(\"✅ 모델 설정 로드 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625dfdeb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 2. (추론) 추론 함수 정의\n",
    "\n",
    "def prepare_inference_data(current_time, feature_matrix, time_index):\n",
    "    \"\"\"\n",
    "    현재 시간을 기준으로 MST-GCN 입력 데이터 준비\n",
    "    \n",
    "    Parameters:\n",
    "    current_time: 현재 시간 (datetime)\n",
    "    feature_matrix: 전체 피처 행렬 (F, N, T)\n",
    "    time_index: 시간 인덱스\n",
    "    \n",
    "    Returns:\n",
    "    x_hour, x_day, x_week 텐서\n",
    "    \"\"\"\n",
    "    # 현재 시간의 인덱스 찾기\n",
    "    current_idx = time_index.get_loc(current_time)\n",
    "    \n",
    "    # 최근 6시간 데이터\n",
    "    hour_start = current_idx - 6\n",
    "    x_hour = feature_matrix[:, :, hour_start:current_idx]\n",
    "    \n",
    "    # 과거 24시간 데이터\n",
    "    day_start = current_idx - 24\n",
    "    x_day = feature_matrix[:, :, day_start:current_idx]\n",
    "    \n",
    "    # 1주일 전 24시간 데이터\n",
    "    week_start = current_idx - 168\n",
    "    week_end = week_start + 24\n",
    "    \n",
    "    #.....\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}