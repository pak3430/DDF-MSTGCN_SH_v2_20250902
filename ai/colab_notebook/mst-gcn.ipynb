{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91f1b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 0. ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import pickle\n",
    "from time import time\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c28d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 1. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "def scaled_Laplacian(W):\n",
    "    '''\n",
    "    compute \\tilde{L}\n",
    "    '''\n",
    "    assert W.shape[0] == W.shape[1]\n",
    "    D = np.diag(np.sum(W, axis=1))\n",
    "    L = D - W\n",
    "    lambda_max = eigs(L, k=1, which='LR')[0].real\n",
    "    return (2 * L) / lambda_max - np.identity(W.shape[0])\n",
    "\n",
    "def cheb_polynomial(L_tilde, K):\n",
    "    '''\n",
    "    compute a list of chebyshev polynomials from T_0 to T_{K-1}\n",
    "    '''\n",
    "    N = L_tilde.shape[0]\n",
    "    cheb_polynomials = [np.identity(N), L_tilde.copy()]\n",
    "    for i in range(2, K):\n",
    "        cheb_polynomials.append(2 * L_tilde @ cheb_polynomials[i - 1] - cheb_polynomials[i - 2])\n",
    "    return cheb_polynomials\n",
    "\n",
    "def create_multi_scale_sequences(data, hour_len=6, day_len=24, week_len=24, \n",
    "                                output_len=24, week_offset=168):\n",
    "    \"\"\"\n",
    "    Create multi-scale input sequences for MST-GCN\n",
    "    Updated for 1-hour timesteps\n",
    "    \"\"\"\n",
    "    X_hour, X_day, X_week, y = [], [], [], []\n",
    "    \n",
    "    # Start from week_offset to ensure we have enough history\n",
    "    for i in range(week_offset, data.shape[2] - output_len + 1):\n",
    "        # Recent pattern (last 6 hours)\n",
    "        hour_slice = data[:, :, i-hour_len:i]\n",
    "        X_hour.append(hour_slice)\n",
    "        \n",
    "        # Daily pattern (last 24 hours)\n",
    "        day_slice = data[:, :, i-day_len:i]\n",
    "        X_day.append(day_slice)\n",
    "        \n",
    "        # Weekly pattern (same 24 hours from last week)\n",
    "        week_slice = data[:, :, i-week_offset:i-week_offset+week_len]\n",
    "        X_week.append(week_slice)\n",
    "        \n",
    "        # Target (next 24 hours, DRT_Demand_Prob which is index 1)\n",
    "        y_slice = data[1, :, i:i+output_len]  # drt_probability is index 1\n",
    "        y.append(y_slice)\n",
    "    \n",
    "    return np.array(X_hour), np.array(X_day), np.array(X_week), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d06cb9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 2. MST-GCN ëª¨ë¸ ì•„í‚¤í…ì²˜\n",
    "\n",
    "class cheb_conv(nn.Module):\n",
    "    '''\n",
    "    K-order chebyshev graph convolution\n",
    "    '''\n",
    "    def __init__(self, K, cheb_polynomials, in_channels, out_channels):\n",
    "        super(cheb_conv, self).__init__()\n",
    "        self.K = K\n",
    "        self.cheb_polynomials = cheb_polynomials\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.DEVICE = cheb_polynomials[0].device\n",
    "        \n",
    "        self.Theta = nn.ParameterList([\n",
    "            nn.Parameter(torch.FloatTensor(in_channels, out_channels).to(self.DEVICE)) \n",
    "            for _ in range(K)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_of_vertices, in_channels, num_of_timesteps = x.shape\n",
    "        \n",
    "        outputs = []\n",
    "        for time_step in range(num_of_timesteps):\n",
    "            graph_signal = x[:, :, :, time_step]\n",
    "            output = torch.zeros(batch_size, num_of_vertices, self.out_channels).to(self.DEVICE)\n",
    "            \n",
    "            for k in range(self.K):\n",
    "                T_k = self.cheb_polynomials[k]\n",
    "                theta_k = self.Theta[k]\n",
    "                rhs = torch.matmul(T_k, graph_signal)\n",
    "                output = output + torch.matmul(rhs, theta_k)\n",
    "                \n",
    "            outputs.append(output.unsqueeze(-1))\n",
    "            \n",
    "        return F.relu(torch.cat(outputs, dim=-1))\n",
    "\n",
    "\n",
    "class MSTGCN_block(nn.Module):\n",
    "    def __init__(self, DEVICE, in_channels, K, nb_chev_filter, nb_time_filter, \n",
    "                 time_conv_strides, cheb_polynomials, num_of_vertices, num_of_timesteps):\n",
    "        super(MSTGCN_block, self).__init__()\n",
    "        \n",
    "        self.cheb_conv = cheb_conv(K, cheb_polynomials, in_channels, nb_chev_filter)\n",
    "        self.time_conv = nn.Conv2d(nb_chev_filter, nb_time_filter, \n",
    "                                   kernel_size=(1, 3), stride=(1, time_conv_strides), \n",
    "                                   padding=(0, 1))\n",
    "        self.residual_conv = nn.Conv2d(in_channels, nb_time_filter, \n",
    "                                       kernel_size=(1, 1), stride=(1, time_conv_strides))\n",
    "        self.ln = nn.LayerNorm(nb_time_filter)\n",
    "\n",
    "    def forward(self, x):\n",
    "        spatial_gcn = self.cheb_conv(x)\n",
    "        time_conv_output = self.time_conv(spatial_gcn.permute(0, 2, 1, 3))\n",
    "        time_conv_output = time_conv_output.permute(0, 2, 1, 3)\n",
    "        \n",
    "        x_residual = self.residual_conv(x.permute(0, 2, 1, 3))\n",
    "        x_residual = x_residual.permute(0, 2, 1, 3)\n",
    "        \n",
    "        out = F.relu(x_residual + time_conv_output)\n",
    "        out = out.permute(0, 1, 3, 2)\n",
    "        out = self.ln(out)\n",
    "        out = out.permute(0, 1, 3, 2)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class MSTGCN_submodule(nn.Module):\n",
    "    def __init__(self, DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter, \n",
    "                 time_strides, cheb_polynomials, num_for_predict, len_input, num_of_vertices):\n",
    "        super(MSTGCN_submodule, self).__init__()\n",
    "        \n",
    "        self.BlockList = nn.ModuleList()\n",
    "        \n",
    "        self.BlockList.append(\n",
    "            MSTGCN_block(DEVICE, in_channels, K, nb_chev_filter, nb_time_filter, \n",
    "                         time_strides, cheb_polynomials, num_of_vertices, len_input)\n",
    "        )\n",
    "        \n",
    "        for _ in range(nb_block - 1):\n",
    "            self.BlockList.append(\n",
    "                MSTGCN_block(DEVICE, nb_time_filter, K, nb_chev_filter, nb_time_filter, \n",
    "                             1, cheb_polynomials, num_of_vertices, len_input // time_strides)\n",
    "            )\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(int(len_input / time_strides), num_for_predict, \n",
    "                                   kernel_size=(1, nb_time_filter))\n",
    "        self.W = nn.Parameter(torch.FloatTensor(num_of_vertices, num_for_predict))\n",
    "        \n",
    "        self.DEVICE = DEVICE\n",
    "        self.to(DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.BlockList:\n",
    "            x = block(x)\n",
    "            \n",
    "        output = self.final_conv(x.permute(0, 3, 1, 2))\n",
    "        output = output[:, :, :, 0].permute(0, 2, 1)\n",
    "        output = output * self.W\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class MSTGCN(nn.Module):\n",
    "    '''\n",
    "    Multi-Scale Temporal Graph Convolutional Networks\n",
    "    '''\n",
    "    def __init__(self, DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter,\n",
    "                 time_strides, cheb_polynomials, num_for_predict, num_of_vertices,\n",
    "                 len_hour, len_day, len_week):\n",
    "        super(MSTGCN, self).__init__()\n",
    "        \n",
    "        self.num_for_predict = num_for_predict\n",
    "        \n",
    "        self.hour_module = MSTGCN_submodule(\n",
    "            DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter,\n",
    "            time_strides, cheb_polynomials, num_for_predict, len_hour, num_of_vertices\n",
    "        )\n",
    "        \n",
    "        self.day_module = MSTGCN_submodule(\n",
    "            DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter,\n",
    "            time_strides, cheb_polynomials, num_for_predict, len_day, num_of_vertices\n",
    "        )\n",
    "        \n",
    "        self.week_module = MSTGCN_submodule(\n",
    "            DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter,\n",
    "            time_strides, cheb_polynomials, num_for_predict, len_week, num_of_vertices\n",
    "        )\n",
    "\n",
    "    def forward(self, x_hour, x_day, x_week):\n",
    "        hour_output = self.hour_module(x_hour)\n",
    "        day_output = self.day_module(x_day)\n",
    "        week_output = self.week_module(x_week)\n",
    "        \n",
    "        output = hour_output + day_output + week_output\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e17d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "#@title 3. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ (8ê°œì›” í†µí•© ë°ì´í„°)\n\n# Google Colabì—ì„œ í•™ìŠµí•  ë•Œ ì‚¬ìš©í•  ì½”ë“œ\n# ë¡œì»¬ì—ì„œ 8ê°œì›” ë°ì´í„°ë¥¼ í†µí•©í•œ dataset.npz íŒŒì¼ ì‚¬ìš©\n\n# ğŸ”„ ìˆ˜ì •: í†µí•©ëœ 8ê°œì›” ë°ì´í„° ë¡œë”©\nbase_path = '/content/drive/MyDrive/train_dataset/dataset.npz'\ndata = np.load(base_path)\n\nfeature_matrix = data['feature_matrix']  # (5, N, T) - 4ê°œ ì…ë ¥ + 1ê°œ íƒ€ê²Ÿ\nstop_ids = data['stop_ids'] \nadj_mx = data['adj_matrix']\n\nprint(\"âœ… í†µí•©ëœ 8ê°œì›” MST-GCN ë°ì´í„° ë¡œë”© ì™„ë£Œ:\")\nprint(f\"Feature matrix shape: {feature_matrix.shape}\")\nprint(f\"ì •ë¥˜ì¥ ìˆ˜: {len(stop_ids)}\")\nprint(f\"ì¸ì ‘ í–‰ë ¬ shape: {adj_mx.shape}\")\n\n# í”¼ì²˜ êµ¬ì„± í™•ì¸\nprint(\"\\ní”¼ì²˜ êµ¬ì„± (5ê°œ):\")\nprint(\"  [0] normalized_log_boarding_count - Log+Z-score ì •ê·œí™” ìˆ˜ìš”\")\nprint(\"  [1] service_availability - ì„œë¹„ìŠ¤ ê°€ìš©ì„± (0,1,2)\")\nprint(\"  [2] is_rest_day - íœ´ì‹ì¼ ì—¬ë¶€ (0,1)\")  \nprint(\"  [3] normalized_interval - ì •ê·œí™”ëœ ë°°ì°¨ê°„ê²© [0,1]\")\nprint(\"  [4] drt_probability - DRT í™•ë¥  (íƒ€ê²Ÿ)\")\n\n# ì…ë ¥ í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬\ninput_features = feature_matrix[:4, :, :]  # ì²˜ìŒ 4ê°œê°€ ì…ë ¥\ntarget_feature = feature_matrix[4, :, :]   # ë§ˆì§€ë§‰ì´ íƒ€ê²Ÿ\n\nprint(f\"\\nì…ë ¥ í”¼ì²˜ shape: {input_features.shape}\")\nprint(f\"íƒ€ê²Ÿ í”¼ì²˜ shape: {target_feature.shape}\")\n\n# ì¸ì ‘ í–‰ë ¬ í†µê³„\nedge_count = np.sum(adj_mx) / 2\navg_degree = np.sum(adj_mx, axis=1).mean()\nprint(f\"\\nì¸ì ‘ í–‰ë ¬ í†µê³„:\")\nprint(f\"ì´ ì—£ì§€ ìˆ˜: {edge_count}\")\nprint(f\"í‰ê·  ì°¨ìˆ˜: {avg_degree:.2f}\")\n\n# í”¼ì²˜ë³„ í†µê³„ ì¶œë ¥\nfor i, feature_name in enumerate(['norm_log_boarding', 'service_avail', 'is_rest_day', 'norm_interval']):\n    feat_data = input_features[i]\n    print(f\"\\n{feature_name} í†µê³„:\")\n    print(f\"  ë²”ìœ„: [{feat_data.min():.4f}, {feat_data.max():.4f}]\")\n    print(f\"  í‰ê· : {feat_data.mean():.4f}\")\n    print(f\"  í‘œì¤€í¸ì°¨: {feat_data.std():.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b0bbfa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "#@title 4. í•™ìŠµ ì‹œí€€ìŠ¤ ë¡œë”© (í†µí•©ëœ 8ê°œì›” ë°ì´í„°ì—ì„œ ì§ì ‘)\n\n# ë°ì´í„° ë¡œë”(data_preparation/mstgcn_data_loader.py)ê°€\n# ì´ë¯¸ ìƒì„±í•œ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‹œí€€ìŠ¤ë¥¼ í†µí•©ëœ npz íŒŒì¼ì—ì„œ ì§ì ‘ ë¡œë“œí•©ë‹ˆë‹¤.\n# ì´ ì…€ì€ ê¸°ì¡´ì˜ ì‹œí€€ìŠ¤ ìƒì„± ì½”ë“œë¥¼ ëŒ€ì²´í•©ë‹ˆë‹¤.\n\n# Cell 3ì—ì„œ ë¡œë“œí•œ 'data' ê°ì²´ì—ì„œ ê° ë°°ì—´ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\nX_hour = data['X_hour']\nX_day = data['X_day']\nX_week = data['X_week']\ny = data['y']\n\nprint(\"âœ… ì‚¬ì „ ì²˜ë¦¬ëœ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‹œí€€ìŠ¤ë¥¼ í†µí•©ëœ 8ê°œì›” ë°ì´í„°ì—ì„œ ì§ì ‘ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\nprint(\"\\n--- ë¡œë“œëœ ë°ì´í„° Shape ---\")\nprint(f\"X_hour: {X_hour.shape}\")\nprint(f\"X_day:  {X_day.shape}\")\nprint(f\"X_week: {X_week.shape}\")\nprint(f\"y:      {y.shape}\")\n\nprint(f\"\\nğŸ“Š 8ê°œì›” í†µí•© ë°ì´í„° í†µê³„:\")\nprint(f\"ì´ ìƒ˜í”Œ ìˆ˜: {X_hour.shape[0]}\")\nprint(f\"ì •ë¥˜ì¥ ìˆ˜: {X_hour.shape[1]}\")\nprint(f\"ì…ë ¥ í”¼ì²˜ ìˆ˜: {X_hour.shape[2]}\")\nprint(f\"ì˜ˆì¸¡ ê¸¸ì´: {y.shape[2]} ì‹œê°„\")\n\n# ë‹¤ìŒ ì…€ì—ì„œ ì‚¬ìš©í•  ë³€ìˆ˜ë“¤ì´ ì˜¬ë°”ë¥´ê²Œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸\nassert 'adj_mx' in locals() or 'adj_mx' in globals(), \"ì˜¤ë¥˜: 'adj_mx'ê°€ ì´ì „ ì…€ì—ì„œ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\"\nassert X_hour.shape[2] == 4, f\"ì˜¤ë¥˜: ì…ë ¥ í”¼ì²˜ì˜ ìˆ˜ê°€ 4ê°€ ì•„ë‹™ë‹ˆë‹¤. ê°ì§€ëœ ìˆ˜: {X_hour.shape[2]}\"\n\nprint(\"\\nâœ… ë°ì´í„° êµ¬ì¡° ê²€ì¦ ì™„ë£Œ. ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ ê°€ëŠ¥.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414beaf9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "#@title 5. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì • ì •ì˜ (4ê°œ ì…ë ¥ í”¼ì²˜)\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', DEVICE)\n\n# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°\nLEARNING_RATE = 0.001\nEPOCHS = 100\nBATCH_SIZE = 32\n\n# MST-GCN ëª¨ë¸ íŒŒë¼ë¯¸í„°\nK = 3                  # Chebyshev ì°¨ìˆ˜ : ê·¸ë˜í”„ ì»¨ë³¼ë£¨ì…˜ ì—°ì‚° ì‹œ, ê° ë…¸ë“œê°€ ëª‡ í™‰(hop) ë–¨ì–´ì§„ ì´ì›ƒì˜ ì •ë³´ê¹Œì§€ ì°¸ê³ í•  ê²ƒì¸ì§€ë¥¼ ì˜ë¯¸\nnb_block = 2           # ë¸”ë¡ ìˆ˜ : í•˜ë‚˜ì˜ ì„œë¸Œëª¨ë“ˆ(hour, day, week) ì•ˆì— Spatio-Temporal ë¸”ë¡ì„ ëª‡ ê°œë‚˜ ìŒ“ì„ ê²ƒì¸ì§€\nnb_chev_filter = 64    # ê³µê°„ í•„í„° ìˆ˜ : ê·¸ë˜í”„ ì»¨ë³¼ë£¨ì…˜(ê³µê°„)ì„ í†µí•´ ì¶”ì¶œí•  íŠ¹ì§•(í”¼ì²˜)ì˜ ê°œìˆ˜\nnb_time_filter = 64    # ì‹œê°„ í•„í„° ìˆ˜ : ì‹œê°„ì  ì»¨ë³¼ë£¨ì…˜ì„ í†µí•´ ì¶”ì¶œí•  íŠ¹ì§•(í”¼ì²˜)ì˜ ê°œìˆ˜\ntime_strides = 1       # ìŠ¤íŠ¸ë¼ì´ë“œ : ì‹œê°„ì  ì»¨ë³¼ë£¨ì…˜ ì—°ì‚° ì‹œ, ëª‡ ì¹¸ì”© ê±´ë„ˆë›°ë©° ì ìš©í• ì§€ ê²°ì •(1ì¸ ê²½ìš° ëª¨ë“  ì‹œì  ì²´í¬)\n\n# ë°ì´í„° shape ê¸°ë°˜ íŒŒë¼ë¯¸í„°(shapeì— ë”°ë¼ ìë™ ê²°ì •)\nnum_of_vertices = adj_mx.shape[0]\nin_channels = X_hour.shape[2]  # 4ê°œ ì…ë ¥ í”¼ì²˜\nnum_for_predict = y.shape[2]   # 24ì‹œê°„ ì˜ˆì¸¡\nlen_hour = X_hour.shape[3]     # 6ì‹œê°„\nlen_day = X_day.shape[3]       # 24ì‹œê°„\nlen_week = X_week.shape[3]     # 24ì‹œê°„\n\nprint(f\"\\nëª¨ë¸ íŒŒë¼ë¯¸í„° (4ê°œ ì…ë ¥ í”¼ì²˜ ê¸°ì¤€):\")\nprint(f\"ì •ë¥˜ì¥ ìˆ˜: {num_of_vertices}\")\nprint(f\"ì…ë ¥ í”¼ì²˜ ìˆ˜: {in_channels} (normalized_log_boarding_count, service_availability, is_rest_day, normalized_interval)\")\nprint(f\"ì˜ˆì¸¡ ì‹œê°„ ê¸¸ì´: {num_for_predict}\")\nprint(f\"Hour ìŠ¤ì¼€ì¼ ê¸¸ì´: {len_hour}\")\nprint(f\"Day ìŠ¤ì¼€ì¼ ê¸¸ì´: {len_day}\")\nprint(f\"Week ìŠ¤ì¼€ì¼ ê¸¸ì´: {len_week}\")\n\n# ëª¨ë¸ ì…ë ¥ ê²€ì¦\nassert in_channels == 4, f\"ì…ë ¥ í”¼ì²˜ ìˆ˜ê°€ 4ê°œê°€ ì•„ë‹™ë‹ˆë‹¤: {in_channels}\"\nprint(f\"\\nâœ… 4ê°œ ì…ë ¥ í”¼ì²˜ í™•ì¸ ì™„ë£Œ\")\n\n# ê° í”¼ì²˜ì˜ ì˜ë¯¸ ì„¤ëª…\nprint(f\"\\nì…ë ¥ í”¼ì²˜ êµ¬ì„±:\")\nprint(f\"  í”¼ì²˜ 0: normalized_log_boarding_count - Log+Z-score ì •ê·œí™”ëœ ìˆ˜ìš” (ì—°ì†ê°’)\")\nprint(f\"  í”¼ì²˜ 1: service_availability - ì„œë¹„ìŠ¤ ê°€ìš©ì„± (0=ë¹„ìš´í–‰, 1=ì‹œê°„ì™¸, 2=ì‹œê°„ë‚´)\")\nprint(f\"  í”¼ì²˜ 2: is_rest_day - íœ´ì‹ì¼ ì—¬ë¶€ (0=í‰ì¼, 1=íœ´ì¼)\")\nprint(f\"  í”¼ì²˜ 3: normalized_interval - ì •ê·œí™”ëœ ë°°ì°¨ê°„ê²© (0~1)\")\n\n# ë°ì´í„° ë²”ìœ„ ê²€ì¦\nprint(f\"\\në°ì´í„° ë²”ìœ„ ê²€ì¦:\")\nfor i, feature_name in enumerate(['norm_log_boarding', 'service_avail', 'is_rest_day', 'norm_interval']):\n    feat_data = X_hour[:, :, i, :]  # ê° í”¼ì²˜ì˜ ë°ì´í„°\n    print(f\"  {feature_name}: [{feat_data.min():.4f}, {feat_data.max():.4f}]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914f3da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "#@title 6. ë°ì´í„° ë¡œë” ìƒì„± (ğŸš¨ Data Leakage ë°©ì§€ - ì‹œê°„ ìˆœì„œ ê¸°ë°˜ ë¶„í• )\n\n# numpy ë°°ì—´ì„ torch í…ì„œë¡œ ë³€í™˜\nX_hour_tensor = torch.from_numpy(X_hour).type(torch.FloatTensor)\nX_day_tensor = torch.from_numpy(X_day).type(torch.FloatTensor)\nX_week_tensor = torch.from_numpy(X_week).type(torch.FloatTensor)\ny_tensor = torch.from_numpy(y).type(torch.FloatTensor)\n\n# Custom Dataset for multi-scale inputs\nclass MultiScaleDataset(torch.utils.data.Dataset):\n    def __init__(self, X_hour, X_day, X_week, y):\n        self.X_hour = X_hour\n        self.X_day = X_day\n        self.X_week = X_week\n        self.y = y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return self.X_hour[idx], self.X_day[idx], self.X_week[idx], self.y[idx]\n\n# ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±\ndataset = MultiScaleDataset(X_hour_tensor, X_day_tensor, X_week_tensor, y_tensor)\n\n# ğŸš¨ ì¤‘ìš”: ì‹œê°„ ìˆœì„œ ê¸°ë°˜ ë¶„í•  (Data Leakage ë°©ì§€)\n# random_split ëŒ€ì‹  ì‹œê°„ ìˆœì„œë¥¼ ë³´ì¥í•˜ëŠ” ë¶„í•  ì‚¬ìš©\ndataset_size = len(dataset)\ntrain_end = int(dataset_size * 0.7)\nval_end = int(dataset_size * 0.85)\n\n# ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì¸ë±ìŠ¤ ìƒì„±\ntrain_indices = list(range(0, train_end))\nval_indices = list(range(train_end, val_end))\ntest_indices = list(range(val_end, dataset_size))\n\nprint(f\"ğŸš¨ Data Leakage ë°©ì§€ë¥¼ ìœ„í•œ ì‹œê°„ ìˆœì„œ ê¸°ë°˜ ë¶„í• :\")\nprint(f\"   Train: ìƒ˜í”Œ [0:{train_end}] â†’ ì´ˆê¸° 70% ì‹œê°„ëŒ€ ({len(train_indices)} ìƒ˜í”Œ)\")\nprint(f\"   Val:   ìƒ˜í”Œ [{train_end}:{val_end}] â†’ ì¤‘ê°„ 15% ì‹œê°„ëŒ€ ({len(val_indices)} ìƒ˜í”Œ)\")\nprint(f\"   Test:  ìƒ˜í”Œ [{val_end}:{dataset_size}] â†’ ë§ˆì§€ë§‰ 15% ì‹œê°„ëŒ€ ({len(test_indices)} ìƒ˜í”Œ)\")\n\n# Subsetìœ¼ë¡œ ë°ì´í„°ì…‹ ë¶„í• \nfrom torch.utils.data import Subset\ntrain_dataset = Subset(dataset, train_indices)\nval_dataset = Subset(dataset, val_indices)\ntest_dataset = Subset(dataset, test_indices)\n\n# DataLoader ìƒì„± (Trainë§Œ ì…”í”Œ, Val/TestëŠ” ì‹œê°„ ìˆœì„œ ìœ ì§€)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f\"\\nğŸ“Š ìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°:\")\nprint(f\"ì „ì²´: {len(dataset)} ìƒ˜í”Œ (8ê°œì›” í†µí•©)\")\nprint(f\"í•™ìŠµ: {len(train_dataset)} ìƒ˜í”Œ\")\nprint(f\"ê²€ì¦: {len(val_dataset)} ìƒ˜í”Œ\")\nprint(f\"í…ŒìŠ¤íŠ¸: {len(test_dataset)} ìƒ˜í”Œ\")\n\nprint(f\"\\nâœ… ì‹œê³„ì—´ íŠ¹ì„±ì„ ë³´ì¡´í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ë¶„í•  ì™„ë£Œ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c363732",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 7. ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”\n",
    "\n",
    "# ì²´ë¹„ì…°í”„ ë‹¤í•­ì‹ ê³„ì‚°\n",
    "L_tilde = scaled_Laplacian(adj_mx)\n",
    "cheb_polynomials = [torch.from_numpy(i).type(torch.FloatTensor).to(DEVICE) for i in cheb_polynomial(L_tilde, K)]\n",
    "\n",
    "# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "model = MSTGCN(\n",
    "    DEVICE, nb_block, in_channels, K, nb_chev_filter, nb_time_filter,\n",
    "    time_strides, cheb_polynomials, num_for_predict, num_of_vertices,\n",
    "    len_hour, len_day, len_week\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜: MSE (DRT í™•ë¥  ì˜ˆì¸¡ìš©)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì €\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ëª¨ë¸ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    else:\n",
    "        nn.init.uniform_(p)\n",
    "\n",
    "print(\"MST-GCN ëª¨ë¸ êµ¬ì¡°:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a6724",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 8. ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ ë£¨í”„\n",
    "\n",
    "print(\"\\ní•™ìŠµ ì‹œì‘...\")\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = -1\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° í†µê³„ (ì •ê·œí™”ìš©)\n",
    "train_indices = train_dataset.indices\n",
    "X_hour_train = X_hour_tensor[train_indices]\n",
    "X_day_train = X_day_tensor[train_indices]\n",
    "X_week_train = X_week_tensor[train_indices]\n",
    "\n",
    "# ê° ìŠ¤ì¼€ì¼ë³„ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°\n",
    "hour_mean = X_hour_train.mean()\n",
    "hour_std = X_hour_train.std()\n",
    "day_mean = X_day_train.mean()\n",
    "day_std = X_day_train.std()\n",
    "week_mean = X_week_train.mean()\n",
    "week_std = X_week_train.std()\n",
    "\n",
    "print(f\"ì •ê·œí™” í†µê³„:\")\n",
    "print(f\"Hour - Mean: {hour_mean:.4f}, Std: {hour_std:.4f}\")\n",
    "print(f\"Day - Mean: {day_mean:.4f}, Std: {day_std:.4f}\")\n",
    "print(f\"Week - Mean: {week_mean:.4f}, Std: {week_std:.4f}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # í•™ìŠµ ë‹¨ê³„\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for x_hour, x_day, x_week, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "        x_hour, x_day, x_week, y_batch = x_hour.to(DEVICE), x_day.to(DEVICE), x_week.to(DEVICE), y_batch.to(DEVICE)\n",
    "        \n",
    "        # ìˆœì „íŒŒ\n",
    "        output = model(x_hour, x_day, x_week)\n",
    "        loss = loss_fn(output, y_batch)\n",
    "        \n",
    "        # ì—­ì „íŒŒ\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    \n",
    "    # ê²€ì¦ ë‹¨ê³„\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_hour, x_day, x_week, y_val in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            x_hour, x_day, x_week, y_val = x_hour.to(DEVICE), x_day.to(DEVICE), x_week.to(DEVICE), y_val.to(DEVICE)\n",
    "            output = model(x_hour, x_day, x_week)\n",
    "            loss = loss_fn(output, y_val)\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), 'best_mstgcn_model.pth')\n",
    "        print(f\"-> Best model saved at epoch {epoch+1} with validation loss {best_val_loss:.6f}\")\n",
    "\n",
    "print(\"\\ní•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"Best validation loss {best_val_loss:.6f} at epoch {best_epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b72a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "#@title 9. TorchServe ë°°í¬ë¥¼ ìœ„í•œ ëª¨ë¸ ë° ì•„í‹°íŒ©íŠ¸ ì €ì¥ (8ê°œì›” í•™ìŠµ ëª¨ë¸)\n\nsave_dir = '/content/drive/MyDrive/MSTGCN_results_8months'\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\n# 1. ì „ì²´ ëª¨ë¸ ì €ì¥ (.pt íŒŒì¼)\nmodel.load_state_dict(torch.load('best_mstgcn_model.pth'))\nmodel.eval()\nfull_model_path = os.path.join(save_dir, 'mstgcn_model_full_8months.pt')\ntorch.save(model, full_model_path)\n\n# 2. ì²´ë¹„ì…°í”„ ë‹¤í•­ì‹ ì €ì¥\ncheb_path = os.path.join(save_dir, 'cheb_polynomials.pt')\ncheb_cpu = [poly.cpu() for poly in cheb_polynomials]\ntorch.save(cheb_cpu, cheb_path)\n\n# 3. ì¸ì ‘ í–‰ë ¬ ì €ì¥\nadj_path = os.path.join(save_dir, 'adjacency_matrix.npy')\nnp.save(adj_path, adj_mx)\n\n# 4. ì •ê·œí™” í†µê³„ ì €ì¥\nstats_path = os.path.join(save_dir, 'normalization_stats.npz')\nnp.savez(stats_path, \n         hour_mean=hour_mean.numpy(), hour_std=hour_std.numpy(),\n         day_mean=day_mean.numpy(), day_std=day_std.numpy(),\n         week_mean=week_mean.numpy(), week_std=week_std.numpy())\n\n# 5. ì •ë¥˜ì¥ ë§¤í•‘ ì •ë³´ ì €ì¥ (8ê°œì›” í†µí•© ë°ì´í„° ê¸°ì¤€)\nstop_mapping_path = os.path.join(save_dir, 'stop_mapping_8months.pkl')\nstop_mapping = {\n    'stop_ids': stop_ids.tolist(),\n    'num_stops': len(stop_ids),\n    'training_period': '2024-11 to 2025-06 (8 months)',\n    'common_stops_only': True\n}\nwith open(stop_mapping_path, 'wb') as f:\n    pickle.dump(stop_mapping, f)\n\n# 6. ëª¨ë¸ ì„¤ì • ì €ì¥ (8ê°œì›” ë°ì´í„° ê¸°ì¤€)\nconfig_path = os.path.join(save_dir, 'model_config_8months.pkl')\nmodel_config = {\n    'nb_block': nb_block,\n    'in_channels': in_channels,\n    'K': K,\n    'nb_chev_filter': nb_chev_filter,\n    'nb_time_filter': nb_time_filter,\n    'time_strides': time_strides,\n    'num_for_predict': num_for_predict,\n    'num_of_vertices': num_of_vertices,\n    'len_hour': len_hour,\n    'len_day': len_day,\n    'len_week': len_week,\n    'device': str(DEVICE),\n    'training_samples': len(dataset),\n    'training_period': '8 months (2024-11 to 2025-06)',\n    'data_leakage_prevented': True,\n    'temporal_split': True\n}\nwith open(config_path, 'wb') as f:\n    pickle.dump(model_config, f)\n\n# 7. í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥\nhistory_path = os.path.join(save_dir, 'training_history_8months.pkl')\nenhanced_history = {\n    'train_loss': history['train_loss'],\n    'val_loss': history['val_loss'],\n    'best_epoch': best_epoch,\n    'best_val_loss': best_val_loss,\n    'total_epochs': EPOCHS,\n    'batch_size': BATCH_SIZE,\n    'learning_rate': LEARNING_RATE,\n    'training_samples': len(train_dataset),\n    'validation_samples': len(val_dataset),\n    'test_samples': len(test_dataset)\n}\nwith open(history_path, 'wb') as f:\n    pickle.dump(enhanced_history, f)\n\nprint(\"\\nâœ… TorchServe ë°°í¬ìš© íŒŒì¼ ì €ì¥ ì™„ë£Œ (8ê°œì›” í•™ìŠµ ëª¨ë¸):\")\nprint(f\"   - ì „ì²´ ëª¨ë¸: {full_model_path}\")\nprint(f\"   - ì²´ë¹„ì…°í”„ ë‹¤í•­ì‹: {cheb_path}\")\nprint(f\"   - ì¸ì ‘ í–‰ë ¬: {adj_path}\")\nprint(f\"   - ì •ê·œí™” í†µê³„: {stats_path}\")\nprint(f\"   - ì •ë¥˜ì¥ ë§¤í•‘: {stop_mapping_path}\")\nprint(f\"   - ëª¨ë¸ ì„¤ì •: {config_path}\")\nprint(f\"   - í•™ìŠµ íˆìŠ¤í† ë¦¬: {history_path}\")\nprint(f\"\\nğŸ¯ íŠ¹ì§•:\")\nprint(f\"   - 8ê°œì›” ì—°ì† ë°ì´í„°ë¡œ í•™ìŠµ\")\nprint(f\"   - Data Leakage ë°©ì§€ ì ìš©\")\nprint(f\"   - ì‹œê°„ ìˆœì„œ ê¸°ë°˜ ë¶„í• \")\nprint(f\"   - ì´ {len(dataset)} ìƒ˜í”Œ í•™ìŠµ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034aa8e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 10. í…ŒìŠ¤íŠ¸ ì…‹ì—ì„œ ìµœì¢… ì„±ëŠ¥ í‰ê°€\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_hour, x_day, x_week, y_test in tqdm(test_loader, desc=\"Test Evaluation\"):\n",
    "        x_hour, x_day, x_week, y_test = x_hour.to(DEVICE), x_day.to(DEVICE), x_week.to(DEVICE), y_test.to(DEVICE)\n",
    "        output = model(x_hour, x_day, x_week)\n",
    "        loss = loss_fn(output, y_test)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        predictions.append(output.cpu().numpy())\n",
    "        actuals.append(y_test.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"\\ní…ŒìŠ¤íŠ¸ ì…‹ MSE Loss: {avg_test_loss:.6f}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ì…‹ RMSE: {np.sqrt(avg_test_loss):.6f}\")\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "actuals = np.concatenate(actuals, axis=0)\n",
    "test_results_path = os.path.join(save_dir, 'test_predictions.npz')\n",
    "np.savez(test_results_path, predictions=predictions, actuals=actuals)\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {test_results_path}\")\n",
    "\n",
    "print(\"\\nğŸ‰ MST-GCN í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fc949",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cac7f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 1. (ì¶”ë¡ ) ì €ì¥ëœ ëª¨ë¸ ë° ì•„í‹°íŒ©íŠ¸ ë¡œë“œ\n",
    "\n",
    "save_dir = '/content/drive/MyDrive/MSTGCN_results'\n",
    "\n",
    "# 1. ì „ì²´ ëª¨ë¸ ë¡œë“œ\n",
    "model_path = os.path.join(save_dir, 'mstgcn_model_full.pt')\n",
    "model = torch.load(model_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval()\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# 2. ì²´ë¹„ì…°í”„ ë‹¤í•­ì‹ ë¡œë“œ\n",
    "cheb_path = os.path.join(save_dir, 'cheb_polynomials.pt')\n",
    "cheb_polynomials = torch.load(cheb_path)\n",
    "print(\"âœ… ì²´ë¹„ì…°í”„ ë‹¤í•­ì‹ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# 3. ì •ê·œí™” í†µê³„ ë¡œë“œ\n",
    "stats_path = os.path.join(save_dir, 'normalization_stats.npz')\n",
    "stats = np.load(stats_path)\n",
    "hour_mean, hour_std = stats['hour_mean'], stats['hour_std']\n",
    "day_mean, day_std = stats['day_mean'], stats['day_std']\n",
    "week_mean, week_std = stats['week_mean'], stats['week_std']\n",
    "print(\"âœ… ì •ê·œí™” í†µê³„ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# 4. ì •ë¥˜ì¥ ë§¤í•‘ ì •ë³´ ë¡œë“œ\n",
    "stop_mapping_path = os.path.join(save_dir, 'stop_mapping.pkl')\n",
    "with open(stop_mapping_path, 'rb') as f:\n",
    "    stop_mapping = pickle.load(f)\n",
    "stop_ids = stop_mapping['stop_ids']\n",
    "stop_info = stop_mapping['stop_info']\n",
    "print(f\"âœ… ì •ë¥˜ì¥ ë§¤í•‘ ë¡œë“œ ì™„ë£Œ ({len(stop_ids)}ê°œ ì •ë¥˜ì¥)\")\n",
    "\n",
    "# 5. ëª¨ë¸ ì„¤ì • ë¡œë“œ\n",
    "config_path = os.path.join(save_dir, 'model_config.pkl')\n",
    "with open(config_path, 'rb') as f:\n",
    "    model_config = pickle.load(f)\n",
    "print(\"âœ… ëª¨ë¸ ì„¤ì • ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625dfdeb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@title 2. (ì¶”ë¡ ) ì¶”ë¡  í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "def prepare_inference_data(current_time, feature_matrix, time_index):\n",
    "    \"\"\"\n",
    "    í˜„ì¬ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ MST-GCN ì…ë ¥ ë°ì´í„° ì¤€ë¹„\n",
    "    \n",
    "    Parameters:\n",
    "    current_time: í˜„ì¬ ì‹œê°„ (datetime)\n",
    "    feature_matrix: ì „ì²´ í”¼ì²˜ í–‰ë ¬ (F, N, T)\n",
    "    time_index: ì‹œê°„ ì¸ë±ìŠ¤\n",
    "    \n",
    "    Returns:\n",
    "    x_hour, x_day, x_week í…ì„œ\n",
    "    \"\"\"\n",
    "    # í˜„ì¬ ì‹œê°„ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
    "    current_idx = time_index.get_loc(current_time)\n",
    "    \n",
    "    # ìµœê·¼ 6ì‹œê°„ ë°ì´í„°\n",
    "    hour_start = current_idx - 6\n",
    "    x_hour = feature_matrix[:, :, hour_start:current_idx]\n",
    "    \n",
    "    # ê³¼ê±° 24ì‹œê°„ ë°ì´í„°\n",
    "    day_start = current_idx - 24\n",
    "    x_day = feature_matrix[:, :, day_start:current_idx]\n",
    "    \n",
    "    # 1ì£¼ì¼ ì „ 24ì‹œê°„ ë°ì´í„°\n",
    "    week_start = current_idx - 168\n",
    "    week_end = week_start + 24\n",
    "    \n",
    "    #.....\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}