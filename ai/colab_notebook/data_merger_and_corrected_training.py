#!/usr/bin/env python3
"""
8ê°œì›” ë°ì´í„° í†µí•© ë° ì‹œê°„ ìˆœì„œ ê¸°ë°˜ ë°ì´í„° ë¶„í•  ìŠ¤í¬ë¦½íŠ¸
Google Colabìš© MST-GCN í•™ìŠµ ë°ì´í„° ì¤€ë¹„

ìˆ˜ì •ì‚¬í•­:
1. 8ê°œì›” ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í†µí•©
2. ì‹œê°„ ìˆœì„œ ê¸°ë°˜ train/val/test ë¶„í•  (Data Leakage ë°©ì§€)
3. ë°ì´í„° ê²½ë¡œë¥¼ '/content/drive/MyDrive/train_dataset/dataset.npz'ë¡œ ë³€ê²½
"""

import numpy as np
import os
from typing import List, Dict, Tuple

def find_common_stops(monthly_dir: str) -> np.ndarray:
    """ëª¨ë“  ì›”ë³„ íŒŒì¼ì—ì„œ ê³µí†µ ì •ë¥˜ì¥ ì°¾ê¸°"""
    npz_files = sorted([f for f in os.listdir(monthly_dir) if f.endswith('.npz')])
    
    common_stops = None
    for file in npz_files:
        file_path = os.path.join(monthly_dir, file)
        data = np.load(file_path)
        stop_ids = data['stop_ids']
        
        if common_stops is None:
            common_stops = set(stop_ids)
        else:
            common_stops = common_stops.intersection(set(stop_ids))
    
    return np.array(sorted(list(common_stops)))

def filter_data_by_stops(data: Dict, common_stops: np.ndarray) -> Dict:
    """ê³µí†µ ì •ë¥˜ì¥ìœ¼ë¡œ ë°ì´í„° í•„í„°ë§"""
    stop_ids = data['stop_ids']
    
    # ê³µí†µ ì •ë¥˜ì¥ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
    common_indices = []
    for stop in common_stops:
        idx = np.where(stop_ids == stop)[0]
        if len(idx) > 0:
            common_indices.append(idx[0])
    
    common_indices = np.array(common_indices)
    
    # ë°ì´í„° í•„í„°ë§
    filtered_data = {}
    for key in data.keys():
        if key in ['X_hour', 'X_day', 'X_week', 'y']:
            # ì‹œê³„ì—´ ë°ì´í„°: (samples, nodes, ...) êµ¬ì¡°
            filtered_data[key] = data[key][:, common_indices, ...]
        elif key in ['feature_matrix']:
            # í”¼ì²˜ í–‰ë ¬: (features, nodes, time) êµ¬ì¡°
            filtered_data[key] = data[key][:, common_indices, :]
        elif key in ['adj_matrix']:
            # ì¸ì ‘ í–‰ë ¬: (nodes, nodes) êµ¬ì¡°
            filtered_data[key] = data[key][np.ix_(common_indices, common_indices)]
        elif key == 'stop_ids':
            filtered_data[key] = common_stops
        else:
            filtered_data[key] = data[key]
    
    return filtered_data

def merge_monthly_data(monthly_dir: str, output_path: str) -> None:
    """8ê°œì›” ë°ì´í„°ë¥¼ ì‹œê°„ ìˆœì„œëŒ€ë¡œ í†µí•©"""
    
    print("ğŸ”„ 8ê°œì›” ë°ì´í„° í†µí•© ì‹œì‘...")
    
    # 1. ê³µí†µ ì •ë¥˜ì¥ ì°¾ê¸°
    print("1. ê³µí†µ ì •ë¥˜ì¥ ì‹ë³„ ì¤‘...")
    common_stops = find_common_stops(monthly_dir)
    print(f"   ê³µí†µ ì •ë¥˜ì¥ ìˆ˜: {len(common_stops)}ê°œ")
    
    # 2. ì›”ë³„ íŒŒì¼ ë¡œë“œ ë° í•„í„°ë§
    npz_files = sorted([f for f in os.listdir(monthly_dir) if f.endswith('.npz')])
    
    merged_data = {
        'X_hour': [],
        'X_day': [],
        'X_week': [],
        'y': [],
        'feature_matrix': None,
        'stop_ids': common_stops,
        'adj_matrix': None
    }
    
    print("2. ì›”ë³„ ë°ì´í„° ë¡œë“œ ë° í•„í„°ë§ ì¤‘...")
    total_samples = 0
    
    for i, file in enumerate(npz_files):
        file_path = os.path.join(monthly_dir, file)
        print(f"   ì²˜ë¦¬ ì¤‘: {file}")
        
        data = {key: val for key, val in np.load(file_path).items()}
        filtered_data = filter_data_by_stops(data, common_stops)
        
        # ì‹œê³„ì—´ ë°ì´í„° ëˆ„ì 
        merged_data['X_hour'].append(filtered_data['X_hour'])
        merged_data['X_day'].append(filtered_data['X_day'])
        merged_data['X_week'].append(filtered_data['X_week'])
        merged_data['y'].append(filtered_data['y'])
        
        total_samples += filtered_data['X_hour'].shape[0]
        print(f"     ìƒ˜í”Œ ìˆ˜: {filtered_data['X_hour'].shape[0]}")
        
        # ë§ˆì§€ë§‰ íŒŒì¼ì˜ ë©”íƒ€ë°ì´í„° ì‚¬ìš©
        if i == len(npz_files) - 1:
            merged_data['feature_matrix'] = filtered_data['feature_matrix']
            merged_data['adj_matrix'] = filtered_data['adj_matrix']
    
    # 3. ì‹œê°„ì¶•ìœ¼ë¡œ ì—°ê²°
    print("3. ì‹œê°„ì¶•ìœ¼ë¡œ ë°ì´í„° ì—°ê²° ì¤‘...")
    merged_data['X_hour'] = np.concatenate(merged_data['X_hour'], axis=0)
    merged_data['X_day'] = np.concatenate(merged_data['X_day'], axis=0)
    merged_data['X_week'] = np.concatenate(merged_data['X_week'], axis=0)
    merged_data['y'] = np.concatenate(merged_data['y'], axis=0)
    
    print(f"âœ… í†µí•© ì™„ë£Œ:")
    print(f"   ì´ ìƒ˜í”Œ ìˆ˜: {total_samples}")
    print(f"   ì •ë¥˜ì¥ ìˆ˜: {len(common_stops)}")
    print(f"   X_hour shape: {merged_data['X_hour'].shape}")
    print(f"   X_day shape: {merged_data['X_day'].shape}")
    print(f"   X_week shape: {merged_data['X_week'].shape}")
    print(f"   y shape: {merged_data['y'].shape}")
    
    # 4. ì €ì¥
    print(f"4. ì €ì¥ ì¤‘: {output_path}")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    np.savez_compressed(output_path, **merged_data)
    print("âœ… ì €ì¥ ì™„ë£Œ!")

# TimeSeriesDatasetëŠ” Google Colabì—ì„œ torchì™€ í•¨ê»˜ ì •ì˜ë¨

def create_temporal_splits(dataset_size: int, train_ratio: float = 0.7, 
                         val_ratio: float = 0.15) -> Tuple[List[int], List[int], List[int]]:
    """ì‹œê°„ ìˆœì„œ ê¸°ë°˜ ë°ì´í„° ë¶„í•  (Data Leakage ë°©ì§€)"""
    
    train_end = int(dataset_size * train_ratio)
    val_end = int(dataset_size * (train_ratio + val_ratio))
    
    train_indices = list(range(0, train_end))
    val_indices = list(range(train_end, val_end))
    test_indices = list(range(val_end, dataset_size))
    
    print(f"ğŸ“Š ì‹œê°„ ìˆœì„œ ê¸°ë°˜ ë°ì´í„° ë¶„í• :")
    print(f"   Train: ìƒ˜í”Œ [0:{train_end}] ({len(train_indices)} ìƒ˜í”Œ, {len(train_indices)/dataset_size*100:.1f}%)")
    print(f"   Val:   ìƒ˜í”Œ [{train_end}:{val_end}] ({len(val_indices)} ìƒ˜í”Œ, {len(val_indices)/dataset_size*100:.1f}%)")
    print(f"   Test:  ìƒ˜í”Œ [{val_end}:{dataset_size}] ({len(test_indices)} ìƒ˜í”Œ, {len(test_indices)/dataset_size*100:.1f}%)")
    
    return train_indices, val_indices, test_indices

# Google Colabìš© ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜
def prepare_colab_dataset():
    """Google Colabìš© ë°ì´í„° ì¤€ë¹„ (ë¡œì»¬ì—ì„œ ì‹¤í–‰)"""
    
    # ë¡œì»¬ ê²½ë¡œ
    monthly_dir = '/Users/leekyoungsoo/teamProject/DDF-ASTGCN/data/processed/monthly'
    output_dir = '/Users/leekyoungsoo/teamProject/DDF-ASTGCN/colab_dataset'
    output_path = os.path.join(output_dir, 'dataset.npz')
    
    # 8ê°œì›” ë°ì´í„° í†µí•©
    merge_monthly_data(monthly_dir, output_path)
    
    print(f"\nğŸ¯ Google Colabìœ¼ë¡œ ì—…ë¡œë“œí•  íŒŒì¼:")
    print(f"   íŒŒì¼ ê²½ë¡œ: {output_path}")
    print(f"   Colab ì €ì¥ ê²½ë¡œ: /content/drive/MyDrive/train_dataset/dataset.npz")
    print(f"\nğŸ“‹ Colabì—ì„œ ì‚¬ìš©í•  ìˆ˜ì •ëœ Cell 3 ì½”ë“œ:")
    print(f"""
# Google Colab Cell 3 ìˆ˜ì • ë²„ì „
base_path = '/content/drive/MyDrive/train_dataset/dataset.npz'
data = np.load(base_path)

feature_matrix = data['feature_matrix']
stop_ids = data['stop_ids'] 
adj_mx = data['adj_matrix']
X_hour = data['X_hour']
X_day = data['X_day']
X_week = data['X_week']
y = data['y']

print("âœ… í†µí•©ëœ 8ê°œì›” MST-GCN ë°ì´í„° ë¡œë”© ì™„ë£Œ:")
print(f"Feature matrix shape: {{feature_matrix.shape}}")
print(f"ì •ë¥˜ì¥ ìˆ˜: {{len(stop_ids)}}")
print(f"X_hour shape: {{X_hour.shape}}")
print(f"X_day shape: {{X_day.shape}}")
print(f"X_week shape: {{X_week.shape}}")
print(f"y shape: {{y.shape}}")
""")

    print(f"\nğŸ“‹ Colabì—ì„œ ì‚¬ìš©í•  ìˆ˜ì •ëœ Cell 6 ì½”ë“œ (Data Leakage ë°©ì§€):")
    print(f"""
# Google Colab Cell 6 ìˆ˜ì • ë²„ì „ - ì‹œê°„ ìˆœì„œ ê¸°ë°˜ ë¶„í• 
X_hour_tensor = torch.from_numpy(X_hour).type(torch.FloatTensor)
X_day_tensor = torch.from_numpy(X_day).type(torch.FloatTensor)
X_week_tensor = torch.from_numpy(X_week).type(torch.FloatTensor)
y_tensor = torch.from_numpy(y).type(torch.FloatTensor)

# ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±
dataset = MultiScaleDataset(X_hour_tensor, X_day_tensor, X_week_tensor, y_tensor)

# ğŸš¨ ì‹œê°„ ìˆœì„œ ê¸°ë°˜ ë¶„í•  (Data Leakage ë°©ì§€)
dataset_size = len(dataset)
train_end = int(dataset_size * 0.7)
val_end = int(dataset_size * 0.85)

train_indices = list(range(0, train_end))
val_indices = list(range(train_end, val_end))
test_indices = list(range(val_end, dataset_size))

# Subsetìœ¼ë¡œ ë¶„í• 
train_dataset = Subset(dataset, train_indices)
val_dataset = Subset(dataset, val_indices)
test_dataset = Subset(dataset, test_indices)

# DataLoader ìƒì„±
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)  # Trainë§Œ ì…”í”Œ
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

print(f"ğŸ“Š ì‹œê°„ ìˆœì„œ ê¸°ë°˜ ë°ì´í„° ë¶„í•  (Data Leakage ë°©ì§€):")
print(f"ì „ì²´: {{len(dataset)}} ìƒ˜í”Œ")
print(f"í•™ìŠµ: {{len(train_dataset)}} ìƒ˜í”Œ (ì´ˆê¸° 70% ì‹œê°„ëŒ€)")
print(f"ê²€ì¦: {{len(val_dataset)}} ìƒ˜í”Œ (ì¤‘ê°„ 15% ì‹œê°„ëŒ€)")
print(f"í…ŒìŠ¤íŠ¸: {{len(test_dataset)}} ìƒ˜í”Œ (ë§ˆì§€ë§‰ 15% ì‹œê°„ëŒ€)")
""")

if __name__ == "__main__":
    prepare_colab_dataset()