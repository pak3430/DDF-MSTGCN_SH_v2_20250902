#!/usr/bin/env python3
"""
Raw to Processed Data Pipeline
ì„œìš¸ì‹œ ì¸ê°€ ë…¸ì„  ê¸°ì¤€ìœ¼ë¡œ raw ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ì—¬ processed ë°ì´í„° ìƒì„±
"""

import pandas as pd
import os
import logging
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RawToProcessed:
    def __init__(self, raw_dir, processed_dir):
        self.raw_dir = Path(raw_dir)
        self.processed_dir = Path(processed_dir)
        
        # processed ë””ë ‰í† ë¦¬ ìƒì„±
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Raw directory: {self.raw_dir}")
        logger.info(f"Processed directory: {self.processed_dir}")
    
    def load_authorized_route_names(self):
        """1. 202507_authorized_route.csvì˜ ì¸ê°€ ë…¸ì„ ëª…ë“¤ì„ ëª¨ë‘ ì¶”ì¶œ"""
        file_path = self.raw_dir / '202507_authorized_route.csv'
        logger.info(f"Loading authorized route names from {file_path}")
        
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            
            # ë…¸ì„ ëª… ì¶”ì¶œ ë° ì •ë¦¬
            authorized_routes = set(df['ë…¸ì„ ëª…'].str.strip().unique())
            
            logger.info(f"âœ… Loaded {len(authorized_routes)} authorized route names")
            logger.info(f"   Sample routes: {list(authorized_routes)[:10]}")
            
            return authorized_routes
            
        except Exception as e:
            logger.error(f"âŒ Error loading authorized route names: {e}")
            raise
    
    def filter_route_info(self, authorized_routes=None):
        """2. seoul_route_info.csvë¥¼ í•„í„°ë§ (Type 8ë§Œ ì œì™¸)"""
        input_path = self.raw_dir / 'seoul_route_info.csv'
        output_path = self.processed_dir / 'seoul_route_info_filtered.csv'
        
        logger.info(f"Filtering route info from {input_path}")
        
        try:
            df = pd.read_csv(input_path, encoding='utf-8-sig')
            original_count = len(df)
            
            # Type 8 (ê¸°íƒ€) ì œì™¸ í•„í„°ë§
            df = df[df['ë…¸ì„ ìœ í˜•'] != 8]
            
            # ë…¸ì„  íƒ€ì…ë³„ í†µê³„ ë¡œê¹…
            type_counts = df['ë…¸ì„ ìœ í˜•'].value_counts().sort_index()
            type_mapping = {
                1: 'ê°„ì„ ë²„ìŠ¤', 2: 'ì§€ì„ ë²„ìŠ¤', 3: 'ìˆœí™˜ë²„ìŠ¤', 
                4: 'ê´‘ì—­ë²„ìŠ¤', 5: 'ë§ˆì„ë²„ìŠ¤', 6: 'ê³µí•­ë²„ìŠ¤', 7: 'ì‹¬ì•¼ë²„ìŠ¤'
            }
            
            logger.info("ğŸ“Š Route types included:")
            for route_type, count in type_counts.items():
                type_name = type_mapping.get(route_type, f'Type {route_type}')
                logger.info(f"   Type {route_type} ({type_name}): {count} routes")
            
            # ë…¸ì„ ëª… ì¤‘ë³µ ì œê±° (ì²« ë²ˆì§¸ ê²ƒë§Œ ìœ ì§€)
            df = df.drop_duplicates(subset=['ë…¸ì„ ëª…'], keep='first')
            
            filtered_count = len(df)
            unique_routes = df['ë…¸ì„ ëª…'].nunique()
            
            # ì €ì¥
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            
            logger.info(f"âœ… Filtered route info: {original_count} â†’ {filtered_count} records")
            logger.info(f"   Unique route names: {unique_routes}")
            logger.info(f"   Saved to: {output_path}")
            
            # í•„í„°ë§ëœ route_id ëª©ë¡ ë°˜í™˜
            return set(df['ë…¸ì„ ID'].astype(str).unique())
            
        except Exception as e:
            logger.error(f"âŒ Error filtering route info: {e}")
            raise
    
    def filter_route_nodes(self, authorized_route_ids):
        """3. seoul_route_node.csvë¥¼ ì¸ê°€ëœ ë…¸ì„  ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§"""
        input_path = self.raw_dir / 'seoul_route_node.csv'
        output_path = self.processed_dir / 'seoul_route_node_filtered.csv'
        
        logger.info(f"Filtering route-node mappings from {input_path}")
        
        try:
            df = pd.read_csv(input_path, encoding='utf-8-sig')
            original_count = len(df)
            
            # ë…¸ì„ IDë¡œ í•„í„°ë§
            df['ë…¸ì„ ID_str'] = df['ë…¸ì„ ID'].astype(str)
            df = df[df['ë…¸ì„ ID_str'].isin(authorized_route_ids)]
            df = df.drop(columns=['ë…¸ì„ ID_str'])
            
            filtered_count = len(df)
            unique_nodes = df['ë…¸ë“œID'].nunique()
            unique_routes = df['ë…¸ì„ ID'].nunique()
            
            # ì €ì¥
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            
            logger.info(f"âœ… Filtered route-node mappings: {original_count} â†’ {filtered_count} records")
            logger.info(f"   Unique routes: {unique_routes}")
            logger.info(f"   Unique nodes: {unique_nodes}")
            logger.info(f"   Saved to: {output_path}")
            
            # í•„í„°ë§ëœ node_id ëª©ë¡ ë°˜í™˜
            return set(df['ë…¸ë“œID'].astype(str).unique())
            
        except Exception as e:
            logger.error(f"âŒ Error filtering route nodes: {e}")
            raise
    
    def filter_node_info(self, used_node_ids):
        """4. seoul_node_info.csvë¥¼ ì‚¬ìš©ë˜ëŠ” ë…¸ë“œë§Œ í•„í„°ë§"""
        input_path = self.raw_dir / 'seoul_node_info.csv'
        output_path = self.processed_dir / 'seoul_node_info_filtered.csv'
        
        logger.info(f"Filtering node info from {input_path}")
        
        try:
            df = pd.read_csv(input_path, encoding='utf-8-sig')
            original_count = len(df)
            
            # ë…¸ë“œIDë¡œ í•„í„°ë§
            df['ë…¸ë“œID_str'] = df['ë…¸ë“œID'].astype(str)
            df = df[df['ë…¸ë“œID_str'].isin(used_node_ids)]
            df = df.drop(columns=['ë…¸ë“œID_str'])
            
            filtered_count = len(df)
            node_types = df['ë…¸ë“œìœ í˜•'].value_counts().to_dict()
            
            # ì €ì¥
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            
            logger.info(f"âœ… Filtered node info: {original_count} â†’ {filtered_count} records")
            logger.info(f"   Node type distribution: {node_types}")
            logger.info(f"   Saved to: {output_path}")
            
            return filtered_count
            
        except Exception as e:
            logger.error(f"âŒ Error filtering node info: {e}")
            raise
    
    def copy_other_files(self):
        """ë‚˜ë¨¸ì§€ íŒŒì¼ë“¤ ê·¸ëŒ€ë¡œ ë³µì‚¬"""
        files_to_copy = [
            'HangJeongDong_ver20250401.geojson'
        ]
        
        for filename in files_to_copy:
            input_path = self.raw_dir / filename
            output_path = self.processed_dir / filename
            
            if input_path.exists():
                # íŒŒì¼ ë³µì‚¬ (pandas ëŒ€ì‹  ì§ì ‘ ë³µì‚¬)
                import shutil
                shutil.copy2(input_path, output_path)
                logger.info(f"âœ… Copied {filename}")
            else:
                logger.warning(f"âš ï¸  File not found: {filename}")
    
    def print_summary(self, authorized_routes, route_ids, node_ids):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        logger.info("=" * 60)
        logger.info("PROCESSING SUMMARY")
        logger.info("=" * 60)
        logger.info(f"ğŸ“‹ Authorized route names: {len(authorized_routes)}")
        logger.info(f"ğŸšŒ Filtered route IDs: {len(route_ids)}")
        logger.info(f"ğŸš Filtered node IDs: {len(node_ids)}")
        logger.info("=" * 60)
        
        # í†µê³„ ì •ë³´ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        summary = {
            'authorized_routes_count': len(authorized_routes),
            'filtered_route_ids_count': len(route_ids),
            'filtered_node_ids_count': len(node_ids),
            'authorized_routes_sample': list(authorized_routes)[:20]
        }
        
        import json
        summary_path = self.processed_dir / 'filter_summary.json'
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“Š Summary saved to: {summary_path}")
    
    def print_summary_v2(self, route_ids, node_ids):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (Type 8 ì œì™¸ ë°©ì‹)"""
        logger.info("=" * 60)
        logger.info("PROCESSING SUMMARY (Type 8 excluded)")
        logger.info("=" * 60)
        logger.info(f"ğŸšŒ Filtered route IDs: {len(route_ids)}")
        logger.info(f"ğŸš Filtered node IDs: {len(node_ids)}")
        logger.info("=" * 60)
        
        # í†µê³„ ì •ë³´ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        summary = {
            'filter_method': 'exclude_type_8',
            'filtered_route_ids_count': len(route_ids),
            'filtered_node_ids_count': len(node_ids)
        }
        
        import json
        summary_path = self.processed_dir / 'filter_summary.json'
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“Š Summary saved to: {summary_path}")
    
    def run(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Type 8 ì œì™¸ í•„í„°ë§)"""
        logger.info("ğŸš€ Starting Raw to Processed Pipeline (excluding Type 8 routes)...")
        
        try:
            # 1. seoul_route_info.csv í•„í„°ë§ (Type 8 ì œì™¸)
            authorized_route_ids = self.filter_route_info()
            
            # 2. seoul_route_node.csv í•„í„°ë§
            used_node_ids = self.filter_route_nodes(authorized_route_ids)
            
            # 3. seoul_node_info.csv í•„í„°ë§
            self.filter_node_info(used_node_ids)
            
            # 4. ë‚˜ë¨¸ì§€ íŒŒì¼ ë³µì‚¬
            self.copy_other_files()
            
            # 5. ìš”ì•½ ì¶œë ¥
            self.print_summary_v2(authorized_route_ids, used_node_ids)
            
            logger.info("âœ… Raw to Processed Pipeline completed successfully!")
            
        except Exception as e:
            logger.error(f"âŒ Pipeline failed: {e}")
            raise


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    base_dir = sys.argv[1] if len(sys.argv) > 1 else '/Users/leekyoungsoo/teamProject/DDF-ASTGCN/data'
    
    raw_dir = os.path.join(base_dir, 'raw/busInfra')
    processed_dir = os.path.join(base_dir, 'processed/busInfra')
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = RawToProcessed(raw_dir, processed_dir)
    pipeline.run()


if __name__ == "__main__":
    main()