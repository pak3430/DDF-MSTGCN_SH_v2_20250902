# traffic_etl.py
# Seoul Traffic Data ETL Pipeline for Historical Traffic Analysis
# Fetches data from 5 APIs and loads into TimescaleDB with Tall Table structure

import requests
import pandas as pd
import psycopg2
from psycopg2.extras import execute_batch, execute_values, RealDictCursor
from psycopg2 import pool
import os
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Set
import time
import traceback
import gc
from math import ceil
import threading
from contextlib import contextmanager
import concurrent.futures
from queue import Queue

# psutilì„ ì„ íƒì ìœ¼ë¡œ import (ì—†ì–´ë„ ë™ì‘í•˜ë„ë¡)
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SeoulTrafficETL:
    """ì„œìš¸ì‹œ êµí†µ ë°ì´í„° ETL íŒŒì´í”„ë¼ì¸ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)"""
    
    def __init__(self, db_config: Dict):
        self.db_config = db_config
        self.conn = None
        self.cur = None
        
        # ì—°ê²° í’€ ì´ˆê¸°í™”
        self.connection_pool = None
        self._init_connection_pool()
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë¡œì»¬ ìŠ¤í† ë¦¬ì§€
        self.local = threading.local()
        
        # ì„±ëŠ¥ ìµœì í™”ëœ ë°°ì¹˜ ì„¤ì •
        self.max_workers = 4  # ë™ì‹œ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜
        
        # API ì„¤ì • (.envì—ì„œ ë¡œë“œ)
        self.api_config = {
            'base_url': os.getenv('SEOUL_API_BASE_URL', 'https://t-data.seoul.go.kr/apig/apiman-gateway/tapi'),
            'timeout': int(os.getenv('API_TIMEOUT', 30)),
            'max_retries': int(os.getenv('API_MAX_RETRIES', 3)),
            'apis': {
                'API1': {
                    'name': 'API1_STATION_PASSENGER',
                    'endpoint': os.getenv('API1_ENDPOINT', 'TaimsTpssStaRouteInfoH/1.0'),
                    'key': os.getenv('API1_STATION_RIDERSHIP_KEY'),
                    'table': 'station_passenger_history'
                },
                'API2': {
                    'name': 'API2_SECTION_PASSENGER', 
                    'endpoint': os.getenv('API2_ENDPOINT', 'TaimsTpssA18RouteSection/1.0'),
                    'key': os.getenv('API2_SECTION_RIDERSHIP_KEY'),
                    'table': 'section_passenger_history'
                },
                'API3': {
                    'name': 'API3_EMD_OD',
                    'endpoint': os.getenv('API3_ENDPOINT', 'TaimsTpssEmdOdTc/1.0'), 
                    'key': os.getenv('API3_EMD_OD_KEY'),
                    'table': 'od_traffic_history'
                },
                'API4': {
                    'name': 'API4_SECTION_SPEED',
                    'endpoint': os.getenv('API4_ENDPOINT', 'TaimsTpssRouteSectionSpeedH/1.0'),
                    'key': os.getenv('API4_SECTION_SPEED_KEY'), 
                    'table': 'section_speed_history'
                },
            }
        }
        
        # ì„±ëŠ¥ ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸° ì„¤ì • (16GB ë©”ëª¨ë¦¬ ê¸°ì¤€)
        self.api_batch_size = 25000    # API í˜¸ì¶œë‹¹ ë ˆì½”ë“œ ìˆ˜ (2.5ë°° ì¦ê°€)
        self.db_batch_size = 20000     # DB ì‚½ì… ë°°ì¹˜ í¬ê¸° (20ë°° ì¦ê°€)
        self.chunk_size = 10000        # Tall Table ë³€í™˜ ì²­í¬ í¬ê¸° (20ë°° ì¦ê°€)
        
        # ì»¤ë°‹ ìµœì í™” ì„¤ì •
        self.commit_batch_count = 3    # Nê°œ ë°°ì¹˜ë§ˆë‹¤ commit
        self.batch_counter = 0
        
        # API í˜¸ì¶œ íšŸìˆ˜ ì¶”ì 
        self.api_call_counts = {
            'API1': 0, 'API2': 0, 'API3': 0, 'API4': 0
        }
        
        # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë‚ ì§œ ì¶”ì 
        self.current_processing_date = None
        
        # ì„œìš¸ì‹œ ë…¸ì„  ID ìºì‹œ (Seoul Route Filtering)
        self.seoul_route_ids: Set[str] = set()
        self.seoul_route_names: Set[str] = set()
        self.route_id_to_name: Dict[str, str] = {}
        
        # í•„í„°ë§ í†µê³„ ì¶”ê°€
        self.filter_stats = {
            'API1': {'total_fetched': 0, 'seoul_filtered': 0},
            'API2': {'total_fetched': 0, 'seoul_filtered': 0},
            'API4': {'total_fetched': 0, 'seoul_filtered': 0}
        }
    
    def _init_connection_pool(self):
        """ì—°ê²° í’€ ì´ˆê¸°í™”"""
        try:
            self.connection_pool = psycopg2.pool.ThreadedConnectionPool(
                minconn=2,      # ìµœì†Œ ì—°ê²° ìˆ˜
                maxconn=8,      # ìµœëŒ€ ì—°ê²° ìˆ˜ (PostgreSQL max_connections ê³ ë ¤)
                **self.db_config
            )
            logger.info("ğŸ”— Database connection pool initialized (2-8 connections)")
        except Exception as e:
            logger.error(f"Connection pool initialization failed: {e}")
            raise
    
    @contextmanager
    def get_db_connection(self):
        """ì—°ê²° í’€ì—ì„œ ì•ˆì „í•œ ì—°ê²° íšë“"""
        conn = None
        try:
            conn = self.connection_pool.getconn()
            yield conn, conn.cursor(cursor_factory=RealDictCursor)
        except Exception as e:
            if conn:
                conn.rollback()
            logger.error(f"Database connection error: {e}")
            raise
        finally:
            if conn:
                self.connection_pool.putconn(conn)
        
    def connect_db(self):
        """ê¸°ì¡´ ë©”ì„œë“œ í˜¸í™˜ì„± ìœ ì§€ (ë ˆê±°ì‹œ ì§€ì›)"""
        try:
            self.conn = psycopg2.connect(**self.db_config)
            self.cur = self.conn.cursor(cursor_factory=RealDictCursor)
            logger.info("Database connected successfully")
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            raise
            
    def close_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ (ì—°ê²° í’€ í¬í•¨)"""
        if self.cur:
            self.cur.close()
        if self.conn:
            self.conn.close()
        if self.connection_pool:
            self.connection_pool.closeall()
            logger.info("Database connection pool closed")
        logger.info("Database connection closed")
    
    def load_seoul_routes(self):
        """DBì—ì„œ ì„œìš¸ì‹œ ë²„ìŠ¤ ë…¸ì„  ì •ë³´ ë¡œë“œ (Seoul Route Filtering)"""
        try:
            logger.info("ğŸšŒ Loading Seoul bus route information from database...")
            
            query = """
                SELECT route_id, route_name 
                FROM bus_routes 
                ORDER BY route_id
            """
            
            self.cur.execute(query)
            routes = self.cur.fetchall()
            
            self.seoul_route_ids = set()
            self.seoul_route_names = set()
            self.route_id_to_name = {}
            
            for route in routes:
                route_id = route['route_id']
                route_name = route['route_name']
                
                self.seoul_route_ids.add(route_id)
                self.seoul_route_names.add(route_name)
                self.route_id_to_name[route_id] = route_name
            
            logger.info(f"âœ… Loaded {len(self.seoul_route_ids)} Seoul bus routes for filtering")
            logger.info(f"   Route ID range: {min(self.seoul_route_ids)} ~ {max(self.seoul_route_ids)}")
            logger.info(f"   Sample routes: {list(self.seoul_route_ids)[:5]}")
            
        except Exception as e:
            logger.error(f"Failed to load Seoul routes: {e}")
            raise
    
    def is_seoul_route(self, route_id: str, route_name: str = None) -> bool:
        """ë…¸ì„ ì´ ì„œìš¸ì‹œ ë…¸ì„ ì¸ì§€ í™•ì¸ (Seoul Route Filtering)"""
        # route_id ê¸°ì¤€ ìš°ì„  í™•ì¸
        if route_id in self.seoul_route_ids:
            return True
        
        # route_name ê¸°ì¤€ ë³´ì¡° í™•ì¸ (API1ì—ì„œ ì‚¬ìš©)
        if route_name and route_name in self.seoul_route_names:
            return True
            
        return False
    
    def _monitor_memory(self, stage: str) -> None:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (feature_generator ë°©ì‹)"""
        if PSUTIL_AVAILABLE:
            try:
                process = psutil.Process()
                memory_mb = process.memory_info().rss / 1024 / 1024
                logger.info(f"Memory usage at {stage}: {memory_mb:.2f} MB")
                
                if memory_mb > 2000:  # 2GB ì´ˆê³¼ì‹œ ê²½ê³ 
                    logger.warning(f"High memory usage detected: {memory_mb:.2f} MB")
                    gc.collect()
            except Exception:
                pass  # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹¤íŒ¨í•´ë„ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ëŠ” ê³„ì†
        else:
            logger.info(f"Processing stage: {stage} (memory monitoring disabled - psutil not available)")
            # psutil ì—†ì´ë„ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì€ ìˆ˜í–‰
            gc.collect()
    
    def log_etl_status(self, job_name: str, status: str, records_processed: int = 0, 
                      records_inserted: int = 0, records_updated: int = 0, 
                      error_message: str = None, data_date: str = None):
        """ETL ì‘ì—… ìƒíƒœë¥¼ DBì— ê¸°ë¡"""
        try:
            if status == 'RUNNING':
                sql = """
                    UPDATE etl_job_status 
                    SET status = %s, last_run_start = CURRENT_TIMESTAMP, 
                        records_processed = 0, records_inserted = 0, records_updated = 0,
                        error_message = NULL, data_date = %s, updated_at = CURRENT_TIMESTAMP
                    WHERE job_name = %s
                """
                self.cur.execute(sql, (status, data_date, job_name))
            elif status in ['SUCCESS', 'FAILED']:
                sql = """
                    UPDATE etl_job_status 
                    SET status = %s, last_run_end = CURRENT_TIMESTAMP,
                        records_processed = %s, records_inserted = %s, records_updated = %s,
                        error_message = %s, updated_at = CURRENT_TIMESTAMP
                """
                params = (status, records_processed, records_inserted, records_updated, error_message)
                if status == 'SUCCESS':
                    sql += ", last_success = CURRENT_TIMESTAMP"
                sql += " WHERE job_name = %s"
                params += (job_name,)
                self.cur.execute(sql, params)
            
            self.conn.commit()
        except Exception as e:
            logger.error(f"Failed to log ETL status: {e}")
    
    def log_etl_message(self, job_name: str, log_level: str, message: str, 
                       execution_step: str = None, additional_data: Dict = None):
        """ETL ìƒì„¸ ë¡œê·¸ë¥¼ DBì— ê¸°ë¡"""
        try:
            sql = """
                INSERT INTO etl_job_logs (job_name, log_level, log_message, execution_step, additional_data)
                VALUES (%s, %s, %s, %s, %s)
            """
            self.cur.execute(sql, (job_name, log_level, message, execution_step, 
                                 json.dumps(additional_data) if additional_data else None))
            self.conn.commit()
        except Exception as e:
            logger.error(f"Failed to log ETL message: {e}")
    
    def make_api_request(self, api_key: str, endpoint: str, params: Dict, api_name: str = None) -> Optional[Dict]:
        """Seoul API ìš”ì²­ ë° ì‘ë‹µ ì²˜ë¦¬ (í˜¸ì¶œ íšŸìˆ˜ ì¶”ì  í¬í•¨)"""
        url = f"{self.api_config['base_url']}/{endpoint}"
        
        # API í‚¤ë¥¼ íŒŒë¼ë¯¸í„°ì— ì¶”ê°€ (api_metadata_extractor.py ë°©ì‹)
        params_with_key = params.copy()
        params_with_key['apikey'] = api_key
        
        for attempt in range(self.api_config['max_retries']):
            try:
                response = requests.get(
                    url, 
                    params=params_with_key,
                    timeout=self.api_config['timeout'],
                    verify=False  # SSL ê²€ì¦ ë¹„í™œì„±í™” (api_metadata_extractor.py ë°©ì‹)
                )
                
                # API í˜¸ì¶œ íšŸìˆ˜ ì¹´ìš´íŠ¸ (ì„±ê³µ/ì‹¤íŒ¨ ë¬´ê´€í•˜ê²Œ ì¹´ìš´íŠ¸)
                if api_name and api_name in self.api_call_counts:
                    self.api_call_counts[api_name] += 1
                
                if response.status_code == 200:
                    data = response.json()
                    logger.info(f"API request successful: {endpoint}, attempt {attempt + 1}, total calls: {self.api_call_counts.get(api_name, 'N/A')}")
                    return data
                elif response.status_code == 500:
                    # 500 ì—ëŸ¬ëŠ” ë³´í†µ ë°ì´í„° ì†Œì§„ì„ ì˜ë¯¸í•˜ë¯€ë¡œ ì²« ë²ˆì§¸ ì‹œë„ì—ì„œ ë°”ë¡œ ì¤‘ë‹¨
                    logger.warning(f"API request failed: {response.status_code} (likely end of data), attempt {attempt + 1}, total calls: {self.api_call_counts.get(api_name, 'N/A')}")
                    return None  # ì¬ì‹œë„ ì—†ì´ ë°”ë¡œ None ë°˜í™˜
                else:
                    logger.warning(f"API request failed: {response.status_code}, attempt {attempt + 1}, total calls: {self.api_call_counts.get(api_name, 'N/A')}")
                    
            except Exception as e:
                logger.error(f"API request error: {e}, attempt {attempt + 1}")
                
            if attempt < self.api_config['max_retries'] - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
        
        return None
    
    def process_api1_station_passenger(self, start_date: str, end_date: str) -> int:
        """API 1: ì •ë¥˜ì¥ë³„ ìŠ¹í•˜ì°¨ ì¸ì›ìˆ˜ ì²˜ë¦¬ (Tall Table ë³€í™˜)"""
        api_config = self.api_config['apis']['API1']
        job_name = api_config['name']
        
        try:
            # DB ì—°ê²° í™•ì¸
            if not self.conn or self.conn.closed:
                self.connect_db()
                
            self.log_etl_status(job_name, 'RUNNING', data_date=start_date)
            self.log_etl_message(job_name, 'INFO', f'Starting API1 processing: {start_date} to {end_date}', 'API_CALL')
            self._monitor_memory("API1 start")
            
            current_date = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')
            total_inserted = 0
            total_days = (end_dt - current_date).days + 1
            
            while current_date <= end_dt:
                date_str = current_date.strftime('%Y%m%d')
                self.current_processing_date = date_str  # í˜„ì¬ ì²˜ë¦¬ ë‚ ì§œ ì¶”ì 
                logger.info(f"ğŸ“… Processing date: {date_str} ({current_date.strftime('%Y-%m-%d')})")
                
                # API ìš”ì²­ íŒŒë¼ë¯¸í„° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ê°ì†Œ)
                params = {
                    'stdrDe': date_str,
                    'startRow': 1,
                    'rowCnt': self.api_batch_size
                }
                
                page_num = 1
                daily_inserted = 0
                
                while True:
                    params['startRow'] = (page_num - 1) * self.api_batch_size + 1
                    
                    # API í˜¸ì¶œ (í˜¸ì¶œ íšŸìˆ˜ ì¶”ì )
                    response_data = self.make_api_request(
                        api_config['key'], 
                        api_config['endpoint'], 
                        params,
                        'API1'
                    )
                    
                    if not response_data:
                        self.log_etl_message(job_name, 'ERROR', f'API call failed for date {date_str}', 'API_CALL')
                        break
                    
                    # ë°ì´í„° ì¶”ì¶œ (API ì‘ë‹µì´ ì§ì ‘ ë°°ì—´)
                    try:
                        if isinstance(response_data, list):
                            items = response_data
                        else:
                            items = response_data.get('TaimsTpssStaRouteInfoH', {}).get('row', [])
                        
                        if not items or len(items) == 0:
                            logger.info(f"No more data available for {date_str} at page {page_num}, moving to next date")
                            break
                        
                        # ğŸ¯ ì„œìš¸ì‹œ ë…¸ì„  í•„í„°ë§ (Seoul Route Filtering)
                        seoul_items = []
                        for item in items:
                            route_id = item.get('routeId', '')
                            route_name = item.get('routeNm', '')
                            
                            self.filter_stats['API1']['total_fetched'] += 1
                            
                            if self.is_seoul_route(route_id, route_name):
                                seoul_items.append(item)
                                self.filter_stats['API1']['seoul_filtered'] += 1
                        
                        logger.info(f"  ğŸ“Š Page {page_num}: {len(items)} total â†’ {len(seoul_items)} Seoul routes")
                        
                        if seoul_items:
                            # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²­í¬ë³„ ë³€í™˜ ë° ì¦‰ì‹œ ì‚½ì… (ì„œìš¸ì‹œ ë°ì´í„°ë§Œ)
                            inserted_count = self.process_api1_chunk_streaming(seoul_items, date_str)
                            daily_inserted += inserted_count
                            
                        page_num += 1
                        
                        # í˜ì´ì§€ ì œí•œ ì²´í¬ (ë¬´í•œë£¨í”„ ë°©ì§€)
                        if page_num > 100:  
                            break
                            
                    except Exception as e:
                        self.log_etl_message(job_name, 'ERROR', f'Data processing error for {date_str}: {e}', 'DATA_TRANSFORM')
                        break
                
                total_inserted += daily_inserted
                self.log_etl_message(job_name, 'INFO', f'Processed {date_str}: {daily_inserted} records', 'DB_INSERT')
                current_date += timedelta(days=1)
            
            self.log_etl_status(job_name, 'SUCCESS', records_processed=total_inserted, records_inserted=total_inserted)
            return total_inserted
            
        except Exception as e:
            error_msg = f"API1 processing failed: {str(e)}\n{traceback.format_exc()}"
            self.log_etl_status(job_name, 'FAILED', error_message=error_msg)
            self.log_etl_message(job_name, 'ERROR', error_msg, 'GENERAL')
            raise
    
    def process_api1_chunk_streaming(self, items: List[Dict], date_str: str) -> int:
        """API1 ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²­í¬ë³„ ë³€í™˜ ë° ì¦‰ì‹œ ì‚½ì… (feature_generator ë°©ì‹)"""
        total_inserted = 0
        
        # ì•„ì´í…œì„ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        for i in range(0, len(items), self.chunk_size):
            chunk = items[i:i + self.chunk_size]
            batch_data = []
            
            for item in chunk:
                route_id = item.get('routeId', '')
                node_id = item.get('staId', '')  # station_id â†’ node_idë¡œ ë§¤í•‘
                route_name = item.get('routeNm', '')
                station_name = item.get('staNm', '')
                station_sequence = item.get('staSn', 0)
                
                # 24ì‹œê°„ ë°ì´í„°ë¥¼ Tall Tableë¡œ ë³€í™˜
                for hour in range(24):
                    hour_str = f"{hour:02d}"
                    
                    dispatch_count = int(item.get(f'a05Num{hour_str}h', 0) or 0)
                    ride_passenger = int(item.get(f'ridePnsgerCnt{hour_str}h', 0) or 0) 
                    alight_passenger = int(item.get(f'alghPnsgerCnt{hour_str}h', 0) or 0)
                    
                    batch_data.append((
                        date_str, route_id, node_id, hour,
                        route_name, station_name, station_sequence,
                        dispatch_count, ride_passenger, alight_passenger
                    ))
            
            # ì²­í¬ë³„ ì¦‰ì‹œ ì‚½ì… (ë©”ëª¨ë¦¬ ì ˆì•½)
            if batch_data:
                # ì¤‘ë³µ í‚¤ ê²€ì¦ (ë””ë²„ê¹…ìš©)
                keys_seen = set()
                duplicates = []
                for record in batch_data:
                    key = (record[0], record[1], record[2], record[3])  # date, route_id, node_id, hour
                    if key in keys_seen:
                        duplicates.append(key)
                    keys_seen.add(key)
                
                if duplicates:
                    logger.warning(f"ğŸš¨ Duplicate keys found in API1 batch: {len(duplicates)} duplicates")
                    logger.warning(f"Sample duplicates: {duplicates[:5]}")
                    # ì¤‘ë³µ ì œê±°
                    unique_batch = []
                    seen_keys = set()
                    for record in batch_data:
                        key = (record[0], record[1], record[2], record[3])
                        if key not in seen_keys:
                            unique_batch.append(record)
                            seen_keys.add(key)
                    batch_data = unique_batch
                    logger.info(f"âœ… Deduplicated batch: {len(batch_data)} unique records")
                
                inserted_count = self.insert_station_passenger_batch(batch_data)
                total_inserted += inserted_count
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del batch_data
                
        return total_inserted
    
    def process_api2_chunk_streaming(self, items: List[Dict], date_str: str) -> int:
        """API2 ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²­í¬ë³„ ë³€í™˜ ë° ì¦‰ì‹œ ì‚½ì…"""
        total_inserted = 0
        
        # ì•„ì´í…œì„ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, len(items), self.chunk_size):
            chunk = items[i:i + self.chunk_size]
            batch_data = []
            
            for item in chunk:
                route_id = item.get('routeId', '')
                from_node_id = item.get('fromStaId', '')
                to_node_id = item.get('toStaId', '')
                station_sequence = item.get('staSn', 0)
                
                # 24ì‹œê°„ ë°ì´í„°ë¥¼ Tall Tableë¡œ ë³€í™˜ (ìµœì í™”ëœ ìŠ¤í‚¤ë§ˆ)
                daily_total_passengers = int(item.get('a18SumLoadPsng', 0) or 0)
                
                for hour in range(24):
                    hour_str = f"{hour:02d}"
                    
                    # API2 ê²€ì¦ ì™„ë£Œ: a18SumLoadPsngNum{hour}h í•„ë“œë§Œ ìœ íš¨
                    passenger_count = item.get(f'a18SumLoadPsngNum{hour_str}h')
                    
                    # NULLì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì •ìˆ˜ ë³€í™˜
                    if passenger_count is not None:
                        try:
                            passenger_count = int(passenger_count)
                        except (ValueError, TypeError):
                            passenger_count = None
                    else:
                        passenger_count = None
                    
                    batch_data.append((
                        date_str, route_id, from_node_id, to_node_id, hour, station_sequence,
                        passenger_count, daily_total_passengers
                    ))
            
            # ì²­í¬ë³„ ì¦‰ì‹œ ì‚½ì…
            if batch_data:
                # ì¤‘ë³µ í‚¤ ê²€ì¦ (ë””ë²„ê¹…ìš©)
                keys_seen = set()
                duplicates = []
                for record in batch_data:
                    key = (record[0], record[1], record[2], record[3], record[4])  # date, route_id, from_node_id, to_node_id, hour
                    if key in keys_seen:
                        duplicates.append(key)
                    keys_seen.add(key)
                
                if duplicates:
                    logger.warning(f"ğŸš¨ Duplicate keys found in API2 batch: {len(duplicates)} duplicates")
                    logger.warning(f"Sample duplicates: {duplicates[:5]}")
                    # ì¤‘ë³µ ì œê±°
                    unique_batch = []
                    seen_keys = set()
                    for record in batch_data:
                        key = (record[0], record[1], record[2], record[3], record[4])
                        if key not in seen_keys:
                            unique_batch.append(record)
                            seen_keys.add(key)
                    batch_data = unique_batch
                    logger.info(f"âœ… Deduplicated API2 batch: {len(batch_data)} unique records")
                
                inserted_count = self.insert_section_passenger_batch(batch_data)
                total_inserted += inserted_count
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del batch_data
                
        return total_inserted
    
    def process_api4_chunk_streaming(self, items: List[Dict], date_str: str) -> int:
        """API4 ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²­í¬ë³„ ë³€í™˜ ë° ì¦‰ì‹œ ì‚½ì… (Seoul Route Filtering)"""
        total_inserted = 0
        
        # ì•„ì´í…œì„ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, len(items), self.chunk_size):
            chunk = items[i:i + self.chunk_size]
            batch_data = []
            
            for item in chunk:
                route_id = item.get('routeId', '')
                
                # Seoul Route Filtering - API4ëŠ” route_idë§Œ ì œê³µ
                self.filter_stats['API4']['total_fetched'] += 1
                if not self.is_seoul_route(route_id):
                    continue  # ì„œìš¸ì‹œ ë…¸ì„ ì´ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°
                
                self.filter_stats['API4']['seoul_filtered'] += 1
                
                from_node_id = item.get('fromStaId', '')
                to_node_id = item.get('toStaId', '')
                from_station_sequence = int(item.get('fromStaSn', 0) or 0)
                to_station_sequence = int(item.get('toStaSn', 0) or 0)
                
                # 24ì‹œê°„ ë°ì´í„°ë¥¼ Tall Tableë¡œ ë³€í™˜ (ìœ íš¨í•œ trip_timeë§Œ ì²˜ë¦¬)
                for hour in range(24):
                    hour_str = f"{hour:02d}"
                    
                    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì²˜ë¦¬: trip_time (73.9% ìœ íš¨ìœ¨)
                    trip_time = int(item.get(f'tripTime{hour_str}h', 0) or 0)
                    
                    batch_data.append((
                        date_str, route_id, from_node_id, to_node_id, hour,
                        from_station_sequence, to_station_sequence, trip_time
                    ))
            
            # ì²­í¬ë³„ ì¦‰ì‹œ ì‚½ì…
            if batch_data:
                # ì¤‘ë³µ í‚¤ ê²€ì¦ (API4 PK: record_date, route_id, from_node_id, to_node_id, hour)
                keys_seen = set()
                duplicates = []
                for record in batch_data:
                    key = (record[0], record[1], record[2], record[3], record[4])  # date, route_id, from_node_id, to_node_id, hour
                    if key in keys_seen:
                        duplicates.append(key)
                    keys_seen.add(key)
                
                if duplicates:
                    logger.warning(f"ğŸš¨ Duplicate keys found in API4 batch: {len(duplicates)} duplicates")
                    logger.warning(f"Sample duplicates: {duplicates[:5]}")
                    # ì¤‘ë³µ ì œê±°
                    unique_batch = []
                    seen_keys = set()
                    for record in batch_data:
                        key = (record[0], record[1], record[2], record[3], record[4])
                        if key not in seen_keys:
                            unique_batch.append(record)
                            seen_keys.add(key)
                    batch_data = unique_batch
                    logger.info(f"âœ… Deduplicated API4 batch: {len(batch_data)} unique records")
                
                inserted_count = self.insert_section_speed_batch(batch_data)
                total_inserted += inserted_count
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del batch_data
                
        return total_inserted
    
    def insert_station_passenger_batch(self, batch_data: List[Tuple]) -> int:
        """ì •ë¥˜ì¥ë³„ ìŠ¹í•˜ì°¨ ë°ì´í„° ë°°ì¹˜ ì‚½ì… (ì„±ëŠ¥ ìµœì í™” ë²„ì „)"""
        if not batch_data:
            return 0
            
        # ì—°ê²° í’€ ì‚¬ìš©
        with self.get_db_connection() as (conn, cur):
            sql = """
                INSERT INTO station_passenger_history (
                    record_date, route_id, node_id, hour,
                    route_name, station_name, station_sequence,
                    dispatch_count, ride_passenger, alight_passenger
                ) VALUES %s
                ON CONFLICT (record_date, route_id, node_id, hour)
                DO UPDATE SET
                    route_name = EXCLUDED.route_name,
                    station_name = EXCLUDED.station_name,
                    station_sequence = EXCLUDED.station_sequence,
                    dispatch_count = EXCLUDED.dispatch_count,
                    ride_passenger = EXCLUDED.ride_passenger,
                    alight_passenger = EXCLUDED.alight_passenger
            """
            
            # execute_valuesë¡œ ê³ ì„±ëŠ¥ ë°°ì¹˜ ì‚½ì… (execute_batchë³´ë‹¤ 5-10ë°° ë¹ ë¦„)
            execute_values(cur, sql, batch_data, page_size=self.db_batch_size)
            
            # ë°°ì¹˜ ì»¤ë°‹ ìµœì í™”: Nê°œ ë°°ì¹˜ë§ˆë‹¤ ì»¤ë°‹
            self.batch_counter += 1
            if self.batch_counter % self.commit_batch_count == 0:
                conn.commit()
                logger.info(f"ğŸ”„ Batch commit executed ({self.batch_counter} batches processed)")
            else:
                conn.commit()  # í˜„ì¬ëŠ” ëª¨ë“  ë°°ì¹˜ì—ì„œ ì»¤ë°‹ (ì•ˆì •ì„± ìš°ì„ )
                
            return len(batch_data)
    
    def process_api2_section_passenger(self, start_date: str, end_date: str) -> int:
        """API 2: êµ¬ê°„ë³„ ìŠ¹ê°ìˆ˜ ì²˜ë¦¬ (Tall Table ë³€í™˜)"""
        api_config = self.api_config['apis']['API2']
        job_name = api_config['name']
        
        try:
            # DB ì—°ê²° í™•ì¸
            if not self.conn or self.conn.closed:
                self.connect_db()
                
            self.log_etl_status(job_name, 'RUNNING', data_date=start_date)
            self.log_etl_message(job_name, 'INFO', f'Starting API2 processing: {start_date} to {end_date}', 'API_CALL')
            
            current_date = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')
            total_inserted = 0
            
            while current_date <= end_dt:
                date_str = current_date.strftime('%Y%m%d')
                params = {'stdrDe': date_str, 'startRow': 1, 'rowCnt': self.api_batch_size}
                
                page_num = 1
                daily_inserted = 0
                
                while True:
                    params['startRow'] = (page_num - 1) * self.api_batch_size + 1
                    response_data = self.make_api_request(api_config['key'], api_config['endpoint'], params, 'API2')
                    
                    if not response_data:
                        break
                    
                    try:
                        if isinstance(response_data, list):
                            items = response_data
                        else:
                            items = response_data.get('TaimsTpssA18RouteSection', {}).get('row', [])
                        
                        if not items or len(items) == 0:
                            logger.info(f"No more data available for {date_str} at page {page_num}, moving to next date")
                            break
                        
                        # ğŸ¯ ì„œìš¸ì‹œ ë…¸ì„  í•„í„°ë§ (Seoul Route Filtering)
                        seoul_items = []
                        for item in items:
                            route_id = item.get('routeId', '')
                            
                            self.filter_stats['API2']['total_fetched'] += 1
                            
                            if self.is_seoul_route(route_id):
                                seoul_items.append(item)
                                self.filter_stats['API2']['seoul_filtered'] += 1
                        
                        logger.info(f"  ğŸ“Š API2 Page {page_num}: {len(items)} total â†’ {len(seoul_items)} Seoul routes")
                        
                        if seoul_items:
                            # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²­í¬ë³„ ë³€í™˜ ë° ì¦‰ì‹œ ì‚½ì… (ì„œìš¸ì‹œ ë°ì´í„°ë§Œ)
                            inserted_count = self.process_api2_chunk_streaming(seoul_items, date_str)
                            daily_inserted += inserted_count
                            
                        page_num += 1
                        if page_num > 100:
                            break
                            
                    except Exception as e:
                        self.log_etl_message(job_name, 'ERROR', f'Data processing error for {date_str}: {e}', 'DATA_TRANSFORM')
                        break
                
                total_inserted += daily_inserted
                self.log_etl_message(job_name, 'INFO', f'Processed {date_str}: {daily_inserted} records', 'DB_INSERT')
                current_date += timedelta(days=1)
            
            self.log_etl_status(job_name, 'SUCCESS', records_processed=total_inserted, records_inserted=total_inserted)
            return total_inserted
            
        except Exception as e:
            error_msg = f"API2 processing failed: {str(e)}\n{traceback.format_exc()}"
            self.log_etl_status(job_name, 'FAILED', error_message=error_msg)
            self.log_etl_message(job_name, 'ERROR', error_msg, 'GENERAL')
            raise
    
    def convert_api2_to_tall_table(self, items: List[Dict], date_str: str) -> List[Tuple]:
        """API2 ë°ì´í„°ë¥¼ Tall Table í˜•íƒœë¡œ ë³€í™˜ (ìµœì í™”ëœ ìŠ¤í‚¤ë§ˆ)"""
        batch_data = []
        
        for item in items:
            route_id = item.get('routeId', '')
            from_node_id = item.get('fromStaId', '')
            to_node_id = item.get('toStaId', '')
            station_sequence = item.get('staSn', 0)
            daily_total_passengers = int(item.get('a18SumLoadPsng', 0) or 0)
            
            # 24ì‹œê°„ ë°ì´í„°ë¥¼ Tall Tableë¡œ ë³€í™˜ (ìœ íš¨ í•„ë“œë§Œ)
            for hour in range(24):
                hour_str = f"{hour:02d}"
                
                # API2 ê²€ì¦ ì™„ë£Œ: a18SumLoadPsngNum{hour}h í•„ë“œë§Œ ìœ íš¨
                passenger_count = item.get(f'a18SumLoadPsngNum{hour_str}h')
                
                # NULLì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì •ìˆ˜ ë³€í™˜
                if passenger_count is not None:
                    try:
                        passenger_count = int(passenger_count)
                    except (ValueError, TypeError):
                        passenger_count = None
                else:
                    passenger_count = None
                
                batch_data.append((
                    date_str, route_id, from_node_id, to_node_id, hour, station_sequence,
                    passenger_count, daily_total_passengers
                ))
        
        return batch_data
    
    def insert_section_passenger_batch(self, batch_data: List[Tuple]) -> int:
        """êµ¬ê°„ë³„ ìŠ¹ê°ìˆ˜ ë°ì´í„° ë°°ì¹˜ ì‚½ì… (ì„±ëŠ¥ ìµœì í™” ë²„ì „)"""
        if not batch_data:
            return 0
            
        # ì—°ê²° í’€ ì‚¬ìš©
        with self.get_db_connection() as (conn, cur):
            sql = """
                INSERT INTO section_passenger_history (
                    record_date, route_id, from_node_id, to_node_id, hour, station_sequence,
                    passenger_count, daily_total_passengers
                ) VALUES %s
                ON CONFLICT (record_date, route_id, from_node_id, to_node_id, hour)
                DO UPDATE SET
                    station_sequence = EXCLUDED.station_sequence,
                    passenger_count = EXCLUDED.passenger_count,
                    daily_total_passengers = EXCLUDED.daily_total_passengers
            """
            
            # ê³ ì„±ëŠ¥ ë°°ì¹˜ ì‚½ì…
            execute_values(cur, sql, batch_data, page_size=self.db_batch_size)
            conn.commit()
            
            return len(batch_data)
        return len(batch_data)
    
    def process_api3_emd_od(self, start_date: str, end_date: str) -> int:
        """API 3: í–‰ì •ë™ë³„ OD í†µí–‰ëŸ‰ ì²˜ë¦¬"""
        api_config = self.api_config['apis']['API3']
        job_name = api_config['name']
        
        try:
            # DB ì—°ê²° í™•ì¸
            if not self.conn or self.conn.closed:
                self.connect_db()
                
            self.log_etl_status(job_name, 'RUNNING', data_date=start_date)
            self.log_etl_message(job_name, 'INFO', f'Starting API3 processing: {start_date} to {end_date}', 'API_CALL')
            
            current_date = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')
            total_inserted = 0
            
            while current_date <= end_dt:
                date_str = current_date.strftime('%Y%m%d')
                params = {
                    'stdrDe': date_str, 
                    'emdCd': '1111051',  # ì²­ìš´íš¨ìë™ (í…ŒìŠ¤íŠ¸ìš© ê¸°ë³¸ê°’)
                    'startRow': 1, 
                    'rowCnt': self.api_batch_size
                }
                
                page_num = 1
                daily_inserted = 0
                
                while True:
                    params['startRow'] = (page_num - 1) * self.api_batch_size + 1
                    response_data = self.make_api_request(api_config['key'], api_config['endpoint'], params, 'API3')
                    
                    if not response_data:
                        break
                    
                    try:
                        if isinstance(response_data, list):
                            items = response_data
                        else:
                            items = response_data.get('TaimsTpssEmdOdTc', {}).get('row', [])
                        
                        if not items or len(items) == 0:
                            logger.info(f"No more data available for {date_str} at page {page_num}, moving to next date")
                            break
                            
                        batch_data = self.convert_api3_to_table(items, date_str)
                        if batch_data:
                            inserted_count = self.insert_od_traffic_batch(batch_data)
                            daily_inserted += inserted_count
                            
                        page_num += 1
                        if page_num > 100:
                            break
                            
                    except Exception as e:
                        self.log_etl_message(job_name, 'ERROR', f'Data processing error for {date_str}: {e}', 'DATA_TRANSFORM')
                        break
                
                total_inserted += daily_inserted
                self.log_etl_message(job_name, 'INFO', f'Processed {date_str}: {daily_inserted} records', 'DB_INSERT')
                current_date += timedelta(days=1)
            
            self.log_etl_status(job_name, 'SUCCESS', records_processed=total_inserted, records_inserted=total_inserted)
            return total_inserted
            
        except Exception as e:
            error_msg = f"API3 processing failed: {str(e)}\n{traceback.format_exc()}"
            self.log_etl_status(job_name, 'FAILED', error_message=error_msg)
            self.log_etl_message(job_name, 'ERROR', error_msg, 'GENERAL')
            raise
    
    def convert_api3_to_table(self, items: List[Dict], date_str: str) -> List[Tuple]:
        """API3 ë°ì´í„° ë³€í™˜ (í•„ë“œëª… ìˆ˜ì •: ì‹¤ì œ API ì‘ë‹µ êµ¬ì¡°ì— ë§ì¶¤)"""
        batch_data = []
        
        for item in items:
            start_district = item.get('startSggNm', '')  # ìˆ˜ì •: startSgg â†’ startSggNm
            start_admin_dong = item.get('startEmdNm', '')
            end_district = item.get('endSggNm', '')      # ìˆ˜ì •: endSgg â†’ endSggNm
            end_admin_dong = item.get('endEmdNm', '')
            total_passenger_count = int(item.get('totPsngNum', 0) or 0)  # ìˆ˜ì •: totTc â†’ totPsngNum
            
            batch_data.append((
                date_str, start_district, start_admin_dong, 
                end_district, end_admin_dong, total_passenger_count
            ))
        
        # ì¤‘ë³µ í‚¤ ê²€ì¦ (API3 PK: record_date, start_district, start_admin_dong, end_district, end_admin_dong)
        keys_seen = set()
        duplicates = []
        for record in batch_data:
            key = (record[0], record[1], record[2], record[3], record[4])  # date, start_district, start_admin_dong, end_district, end_admin_dong
            if key in keys_seen:
                duplicates.append(key)
            keys_seen.add(key)
        
        if duplicates:
            logger.warning(f"ğŸš¨ Duplicate keys found in API3 batch: {len(duplicates)} duplicates")
            logger.warning(f"Sample duplicates: {duplicates[:5]}")
            # ì¤‘ë³µ ì œê±°
            unique_batch = []
            seen_keys = set()
            for record in batch_data:
                key = (record[0], record[1], record[2], record[3], record[4])
                if key not in seen_keys:
                    unique_batch.append(record)
                    seen_keys.add(key)
            batch_data = unique_batch
            logger.info(f"âœ… Deduplicated API3 batch: {len(batch_data)} unique records")
        
        return batch_data
    
    def insert_od_traffic_batch(self, batch_data: List[Tuple]) -> int:
        """OD í†µí–‰ëŸ‰ ë°ì´í„° ë°°ì¹˜ ì‚½ì…"""
        if not batch_data:
            return 0
            
        sql = """
            INSERT INTO od_traffic_history (
                record_date, start_district, start_admin_dong,
                end_district, end_admin_dong, total_passenger_count
            ) VALUES (%s, %s, %s, %s, %s, %s)
            ON CONFLICT (record_date, start_district, start_admin_dong, end_district, end_admin_dong)
            DO UPDATE SET
                total_passenger_count = EXCLUDED.total_passenger_count
        """
        
        execute_batch(self.cur, sql, batch_data, page_size=self.db_batch_size)
        self.conn.commit()
        return len(batch_data)
    
    def process_api4_section_speed(self, start_date: str, end_date: str) -> int:
        """API 4: êµ¬ê°„ë³„ ìš´í–‰ì‹œê°„ ì²˜ë¦¬ (Tall Table ë³€í™˜)"""
        api_config = self.api_config['apis']['API4']
        job_name = api_config['name']
        
        try:
            # DB ì—°ê²° í™•ì¸
            if not self.conn or self.conn.closed:
                self.connect_db()
                
            self.log_etl_status(job_name, 'RUNNING', data_date=start_date)
            self.log_etl_message(job_name, 'INFO', f'Starting API4 processing: {start_date} to {end_date}', 'API_CALL')
            
            current_date = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')
            total_inserted = 0
            
            while current_date <= end_dt:
                date_str = current_date.strftime('%Y%m%d')
                params = {'stdrDe': date_str, 'startRow': 1, 'rowCnt': self.api_batch_size}
                
                page_num = 1
                daily_inserted = 0
                
                while True:
                    params['startRow'] = (page_num - 1) * self.api_batch_size + 1
                    response_data = self.make_api_request(api_config['key'], api_config['endpoint'], params, 'API4')
                    
                    if not response_data:
                        break
                    
                    try:
                        if isinstance(response_data, list):
                            items = response_data
                        else:
                            items = response_data.get('TaimsTpssRouteSectionSpeedH', {}).get('row', [])
                        
                        if not items or len(items) == 0:
                            logger.info(f"No more data available for {date_str} at page {page_num}, moving to next date")
                            break
                            
                        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²­í¬ë³„ ë³€í™˜ ë° ì¦‰ì‹œ ì‚½ì…
                        inserted_count = self.process_api4_chunk_streaming(items, date_str)
                        daily_inserted += inserted_count
                            
                        page_num += 1
                        if page_num > 100:
                            break
                            
                    except Exception as e:
                        self.log_etl_message(job_name, 'ERROR', f'Data processing error for {date_str}: {e}', 'DATA_TRANSFORM')
                        break
                
                total_inserted += daily_inserted
                self.log_etl_message(job_name, 'INFO', f'Processed {date_str}: {daily_inserted} records', 'DB_INSERT')
                current_date += timedelta(days=1)
            
            self.log_etl_status(job_name, 'SUCCESS', records_processed=total_inserted, records_inserted=total_inserted)
            return total_inserted
            
        except Exception as e:
            error_msg = f"API4 processing failed: {str(e)}\n{traceback.format_exc()}"
            self.log_etl_status(job_name, 'FAILED', error_message=error_msg)
            self.log_etl_message(job_name, 'ERROR', error_msg, 'GENERAL')
            raise
    
    def convert_api4_to_tall_table(self, items: List[Dict], date_str: str) -> List[Tuple]:
        """API4 ë°ì´í„°ë¥¼ Tall Table í˜•íƒœë¡œ ë³€í™˜"""
        batch_data = []
        
        for item in items:
            route_id = item.get('routeId', '')
            from_node_id = item.get('fromStaId', '')
            to_node_id = item.get('toStaId', '')
            from_station_sequence = int(item.get('fromStaSn', 0) or 0)
            to_station_sequence = int(item.get('toStaSn', 0) or 0)
            usage_count = int(item.get('useCnt', 0) or 0)
            
            # 24ì‹œê°„ ë°ì´í„°ë¥¼ Tall Tableë¡œ ë³€í™˜
            for hour in range(24):
                hour_str = f"{hour:02d}"
                
                # speed = float(item.get(f'speed{hour_str}h', 0) or 0)  # API ì‘ë‹µì—ì„œ ëª¨ë“  ê°’ì´ 0ì´ë¯€ë¡œ ì™„ì „ ì œê±°
                trip_time = int(item.get(f'tripTime{hour_str}h', 0) or 0)
                
                batch_data.append((
                    date_str, route_id, from_node_id, to_node_id, hour,
                    from_station_sequence, to_station_sequence, usage_count,
                    trip_time
                ))
        
        return batch_data
    
    def insert_section_speed_batch(self, batch_data: List[Tuple]) -> int:
        """êµ¬ê°„ë³„ ìš´í–‰ì‹œê°„ ë°ì´í„° ë°°ì¹˜ ì‚½ì…"""
        if not batch_data:
            return 0
            
        sql = """
            INSERT INTO section_speed_history (
                record_date, route_id, from_node_id, to_node_id, hour,
                from_station_sequence, to_station_sequence, trip_time
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (record_date, route_id, from_node_id, to_node_id, hour)
            DO UPDATE SET
                from_station_sequence = EXCLUDED.from_station_sequence,
                to_station_sequence = EXCLUDED.to_station_sequence,
                trip_time = EXCLUDED.trip_time
        """
        
        execute_batch(self.cur, sql, batch_data, page_size=self.db_batch_size)
        self.conn.commit()
        return len(batch_data)
    
    def refresh_materialized_views(self):
        """ETL ì™„ë£Œ í›„ Materialized Views ê°±ì‹  (API ì„±ëŠ¥ ìµœì í™”)"""
        try:
            logger.info("ğŸ”„ Starting materialized view refresh for API optimization...")
            
            # 1. ê¸°ë³¸ MV ê°±ì‹ 
            refresh_sql = "SELECT refresh_all_traffic_views();"
            self.cur.execute(refresh_sql)
            self.conn.commit()
            
            # 2. Anomaly Pattern ì „ìš© MV ê°±ì‹ 
            logger.info("ğŸ¯ Refreshing Anomaly Pattern MV (station_hourly_patterns)...")
            try:
                # MVê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                check_sql = """
                    SELECT EXISTS (
                        SELECT 1 FROM pg_matviews 
                        WHERE matviewname = 'mv_station_hourly_patterns'
                    );
                """
                self.cur.execute(check_sql)
                exists = self.cur.fetchone()[0]
                
                if exists:
                    # MVê°€ ìˆìœ¼ë©´ ê°±ì‹  í•¨ìˆ˜ í˜¸ì¶œ
                    refresh_anomaly_sql = "SELECT refresh_station_hourly_patterns();"
                    self.cur.execute(refresh_anomaly_sql)
                    self.conn.commit()
                    logger.info("âœ… Anomaly Pattern MV refreshed successfully!")
                else:
                    logger.warning("âš ï¸ mv_station_hourly_patterns not found. Please run 002_station_hourly_patterns.sql first.")
                
            except Exception as e:
                logger.warning(f"âš ï¸ Could not refresh Anomaly Pattern MV: {e}")
                # ì‹¤íŒ¨í•´ë„ ì „ì²´ ETLì€ ê³„ì† ì§„í–‰
            
            # ê°±ì‹  í›„ í†µê³„ í™•ì¸
            stats_sql = "SELECT * FROM check_mv_statistics();"
            self.cur.execute(stats_sql)
            stats = self.cur.fetchall()
            
            logger.info("ğŸ“Š Materialized View Statistics:")
            for stat in stats:
                view_name = stat['view_name'] if isinstance(stat, dict) else stat[0]
                row_count = stat['row_count'] if isinstance(stat, dict) else stat[1]
                disk_size = stat['disk_size'] if isinstance(stat, dict) else stat[2]
                logger.info(f"   - {view_name}: {row_count:,} rows, {disk_size}")
            
            # ETL ë¡œê·¸ì— ê¸°ë¡
            self.log_etl_message('ETL_MATERIALIZED_VIEWS', 'INFO', 
                               f'Successfully refreshed {len(stats)} materialized views', 
                               'AGGREGATION',
                               {'refreshed_views': len(stats)})
            
        except Exception as e:
            logger.error(f"Failed to refresh materialized views: {e}")
            # êµ¬ì²´ì ì¸ ì—ëŸ¬ ë¡œê¹…
            error_details = {
                'error_type': type(e).__name__,
                'error_message': str(e),
                'stage': 'materialized_view_refresh'
            }
            self.log_etl_message('ETL_MATERIALIZED_VIEWS', 'ERROR', f'Materialized view refresh failed: {e}', 'AGGREGATION', error_details)
            raise
    
    def run_full_etl(self, start_date: str = '20250719', end_date: str = '20250731'):
        """ì „ì²´ ETL í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ë‚ ì§œë³„ ë£¨í”„ ë°©ì‹)"""
        logger.info(f"=== Starting Seoul Traffic ETL Process (Daily Loop Mode) ===")
        logger.info(f"Date Range: {start_date} to {end_date} (continuing from last complete date: 2025-07-18)")
        logger.info(f"ğŸ“… Processing Pattern: Each date will process API1â†’API2â†’API3â†’API4 sequentially")
        self._monitor_memory("ETL process start")
        
        try:
            self.connect_db()
            
            # Seoul Route Filtering - DBì—ì„œ ì„œìš¸ì‹œ ë…¸ì„  ì •ë³´ ë¡œë“œ
            self.load_seoul_routes()
            
            # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
            current_date = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')
            total_days = (end_dt - current_date).days + 1
            
            # ëˆ„ì  í†µê³„
            total_api1_count = 0
            total_api2_count = 0
            total_api3_count = 0
            total_api4_count = 0
            day_counter = 0
            
            logger.info(f"ğŸ—“ï¸ Total days to process: {total_days}")
            logger.info("="*80)
            
            # ë‚ ì§œë³„ ë£¨í”„ (ê° ë‚ ì§œë§ˆë‹¤ API1-4 ìˆœì°¨ ì²˜ë¦¬)
            while current_date <= end_dt:
                day_counter += 1
                date_str = current_date.strftime('%Y%m%d')
                date_display = current_date.strftime('%Y-%m-%d (%a)')
                
                logger.info(f"ğŸ“… Day {day_counter}/{total_days}: Processing {date_display}")
                logger.info("-" * 60)
                
                daily_start_time = datetime.now()
                
                try:
                    # API1: ì •ë¥˜ì¥ë³„ ìŠ¹í•˜ì°¨ ë°ì´í„° (ë‹¨ì¼ ë‚ ì§œ)
                    logger.info(f"  ğŸ“Š API1: Station Passenger Data for {date_str}")
                    self._monitor_memory(f"Day{day_counter} API1 start")
                    api1_count = self.process_api1_station_passenger(date_str, date_str)
                    total_api1_count += api1_count
                    logger.info(f"  âœ… API1 completed: {api1_count:,} records")
                    
                    # API2: êµ¬ê°„ë³„ ìŠ¹ê°ìˆ˜ ë°ì´í„° (ë‹¨ì¼ ë‚ ì§œ)
                    logger.info(f"  ğŸ“Š API2: Section Passenger Data for {date_str}")
                    self._monitor_memory(f"Day{day_counter} API2 start")
                    api2_count = self.process_api2_section_passenger(date_str, date_str)
                    total_api2_count += api2_count
                    logger.info(f"  âœ… API2 completed: {api2_count:,} records")
                    
                    # API3: í–‰ì •ë™ë³„ OD í†µí–‰ëŸ‰ ë°ì´í„° (ë‹¨ì¼ ë‚ ì§œ)
                    logger.info(f"  ğŸ“Š API3: EMD OD Traffic Data for {date_str}")
                    self._monitor_memory(f"Day{day_counter} API3 start")
                    api3_count = self.process_api3_emd_od(date_str, date_str)
                    total_api3_count += api3_count
                    logger.info(f"  âœ… API3 completed: {api3_count:,} records")
                    
                    # API4: êµ¬ê°„ë³„ ìš´í–‰ì‹œê°„ ë°ì´í„° (ë‹¨ì¼ ë‚ ì§œ)
                    logger.info(f"  ğŸ“Š API4: Section Speed Data for {date_str}")
                    self._monitor_memory(f"Day{day_counter} API4 start")
                    api4_count = self.process_api4_section_speed(date_str, date_str)
                    total_api4_count += api4_count
                    logger.info(f"  âœ… API4 completed: {api4_count:,} records")
                    
                    # ì¼ë³„ ìš”ì•½
                    daily_total = api1_count + api2_count + api3_count + api4_count
                    daily_duration = datetime.now() - daily_start_time
                    logger.info(f"ğŸ¯ Day {day_counter} Summary: {daily_total:,} records in {daily_duration}")
                    logger.info(f"   API1: {api1_count:,} | API2: {api2_count:,} | API3: {api3_count:,} | API4: {api4_count:,}")
                    
                except Exception as e:
                    logger.error(f"âŒ Day {day_counter} ({date_str}) failed: {e}")
                    # ê°œë³„ ë‚ ì§œ ì‹¤íŒ¨ ì‹œì—ë„ ë‹¤ìŒ ë‚ ì§œ ê³„ì† ì§„í–‰
                    continue
                
                logger.info("=" * 60)
                current_date += timedelta(days=1)
            
            # ì „ì²´ ìš”ì•½
            total_records = total_api1_count + total_api2_count + total_api3_count + total_api4_count
            total_api_calls = sum(self.api_call_counts.values())
            
            logger.info("="*80)
            logger.info("ğŸ‰ Daily Loop ETL Process Completed Successfully!")
            logger.info(f"ğŸ“ˆ Total Records Processed: {total_records:,}")
            logger.info(f"   - API1 (Station Passenger): {total_api1_count:,}")
            logger.info(f"   - API2 (Section Passenger): {total_api2_count:,}")
            logger.info(f"   - API3 (EMD OD Traffic): {total_api3_count:,}")
            logger.info(f"   - API4 (Section Speed): {total_api4_count:,}")
            logger.info(f"ğŸ“ Total API Calls Made: {total_api_calls:,} (Rate Limit: 1000/day)")
            logger.info(f"   - API1 Calls: {self.api_call_counts['API1']:,}")
            logger.info(f"   - API2 Calls: {self.api_call_counts['API2']:,}")
            logger.info(f"   - API3 Calls: {self.api_call_counts['API3']:,}")
            logger.info(f"   - API4 Calls: {self.api_call_counts['API4']:,}")
            logger.info(f"ğŸ“… Date Range Processed: {start_date} to {end_date} ({total_days} days)")
            
            # Seoul Route Filtering í†µê³„ ì¶œë ¥
            logger.info("="*80)
            logger.info("ğŸšŒ Seoul Route Filtering Statistics:")
            logger.info(f"   - Seoul Routes Loaded: {len(self.seoul_route_ids):,} routes")
            for api_name, stats in self.filter_stats.items():
                if stats['total_fetched'] > 0:
                    filter_rate = (stats['seoul_filtered'] / stats['total_fetched']) * 100
                    logger.info(f"   - {api_name}: {stats['seoul_filtered']:,}/{stats['total_fetched']:,} records ({filter_rate:.1f}% Seoul routes)")
            
            # âœ… ETL ì™„ë£Œ í›„ Materialized Views ê°±ì‹  (API ì„±ëŠ¥ ìµœì í™”)
            logger.info("="*80)
            logger.info("ğŸ“Š Refreshing Materialized Views for API Optimization...")
            try:
                self.refresh_materialized_views()
                logger.info("âœ… All materialized views refreshed successfully!")
            except Exception as e:
                logger.error(f"âŒ Failed to refresh materialized views: {e}")
                # ì§‘ê³„ í…Œì´ë¸” ê°±ì‹  ì‹¤íŒ¨í•´ë„ ì „ì²´ ETLì€ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                self.log_etl_message('ETL_MATERIALIZED_VIEWS', 'ERROR', f'Materialized view refresh failed: {e}', 'AGGREGATION')
            
            # API í˜¸ì¶œ í†µê³„ë¥¼ DBì—ë„ ê¸°ë¡
            self.log_etl_message('ETL_SUMMARY', 'INFO', f'Daily loop ETL: {total_api_calls} API calls for {start_date}-{end_date}', 'API_STATISTICS', 
                               {'api_call_counts': self.api_call_counts, 'total_records': total_records, 'total_days': total_days, 'date_range': f'{start_date}-{end_date}'})
            logger.info("="*80)
            self._monitor_memory("ETL process completed")
            
        except Exception as e:
            logger.error(f"âŒ Daily Loop ETL process failed: {e}")
            self._monitor_memory("ETL process failed")
            raise
        finally:
            self.close_db()
    
    def run_parallel_etl(self, start_date: str = '20250719', end_date: str = '20250731'):
        """ë³‘ë ¬ ì²˜ë¦¬ ETL í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)"""
        logger.info(f"=== Starting Parallel Seoul Traffic ETL Process ===")
        logger.info(f"Date Range: {start_date} to {end_date} (continuing from last complete date: 2025-07-18)")
        logger.info(f"ğŸš€ Processing Pattern: Parallel execution with {self.max_workers} workers")
        logger.info(f"âš¡ Performance Optimizations: Connection Pool + Batch Processing + Parallel APIs")
        self._monitor_memory("Parallel ETL process start")
        
        try:
            # ê¸°ë³¸ ì—°ê²° ìƒì„± (Seoul Routes ë¡œë”©ìš©)
            self.connect_db()
            self.load_seoul_routes()
            
            # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
            current_date = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')
            total_days = (end_dt - current_date).days + 1
            
            # ë‚ ì§œë³„ ì‘ì—… í ìƒì„±
            date_queue = Queue()
            while current_date <= end_dt:
                date_queue.put(current_date.strftime('%Y%m%d'))
                current_date += timedelta(days=1)
            
            logger.info(f"ğŸ—“ï¸ Total days to process: {total_days}")
            logger.info(f"ğŸ‘¥ Parallel workers: {self.max_workers}")
            logger.info("="*80)
            
            # ë³‘ë ¬ ì²˜ë¦¬ í†µê³„
            total_api1_count = 0
            total_api2_count = 0  
            total_api3_count = 0
            total_api4_count = 0
            processed_dates = []
            
            # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = []
                
                # ê° ë‚ ì§œë³„ë¡œ APIë“¤ì„ ë³‘ë ¬ ì²˜ë¦¬
                while not date_queue.empty():
                    date_str = date_queue.get()
                    processed_dates.append(date_str)
                    
                    logger.info(f"ğŸ“… Queuing parallel processing for {date_str}")
                    
                    # APIë³„ ë³‘ë ¬ ì‹¤í–‰
                    future_api1 = executor.submit(self._process_api1_parallel, date_str)
                    future_api2 = executor.submit(self._process_api2_parallel, date_str)
                    future_api3 = executor.submit(self._process_api3_parallel, date_str)
                    future_api4 = executor.submit(self._process_api4_parallel, date_str)
                    
                    futures.extend([
                        ('API1', date_str, future_api1),
                        ('API2', date_str, future_api2), 
                        ('API3', date_str, future_api3),
                        ('API4', date_str, future_api4)
                    ])
                
                logger.info(f"ğŸ”„ Processing {len(futures)} parallel tasks...")
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for api_name, date_str, future in futures:
                    try:
                        result = future.result(timeout=600)  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                        
                        if api_name == 'API1':
                            total_api1_count += result
                        elif api_name == 'API2':
                            total_api2_count += result
                        elif api_name == 'API3':
                            total_api3_count += result
                        elif api_name == 'API4':
                            total_api4_count += result
                            
                        logger.info(f"âœ… {api_name} for {date_str}: {result:,} records")
                        
                    except concurrent.futures.TimeoutError:
                        logger.error(f"â° {api_name} for {date_str} timed out")
                    except Exception as e:
                        logger.error(f"âŒ {api_name} for {date_str} failed: {e}")
            
            # ì „ì²´ ìš”ì•½
            total_records = total_api1_count + total_api2_count + total_api3_count + total_api4_count
            total_api_calls = sum(self.api_call_counts.values())
            
            logger.info("="*80)
            logger.info("ğŸ‰ Parallel ETL Process Completed Successfully!")
            logger.info(f"ğŸ“ˆ Total Records Processed: {total_records:,}")
            logger.info(f"   - API1 (Station Passenger): {total_api1_count:,}")
            logger.info(f"   - API2 (Section Passenger): {total_api2_count:,}")
            logger.info(f"   - API3 (EMD OD Traffic): {total_api3_count:,}")
            logger.info(f"   - API4 (Section Speed): {total_api4_count:,}")
            logger.info(f"ğŸ“ Total API Calls Made: {total_api_calls:,}")
            logger.info(f"ğŸ“… Date Range Processed: {start_date} to {end_date} ({total_days} days)")
            logger.info(f"âš¡ Parallel Processing: {self.max_workers} workers")
            
            # âœ… ETL ì™„ë£Œ í›„ Materialized Views ê°±ì‹  (API ì„±ëŠ¥ ìµœì í™”)
            logger.info("="*80)
            logger.info("ğŸ“Š Refreshing Materialized Views for API Optimization...")
            try:
                self.refresh_materialized_views()
                logger.info("âœ… All materialized views refreshed successfully!")
            except Exception as e:
                logger.error(f"âŒ Failed to refresh materialized views: {e}")
                # ì§‘ê³„ í…Œì´ë¸” ê°±ì‹  ì‹¤íŒ¨í•´ë„ ì „ì²´ ETLì€ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
            
            logger.info("="*80)
            self._monitor_memory("Parallel ETL process completed")
            
        except Exception as e:
            logger.error(f"âŒ Parallel ETL process failed: {e}")
            self._monitor_memory("Parallel ETL process failed")
            raise
        finally:
            self.close_db()
    
    def _process_api1_parallel(self, date_str: str) -> int:
        """API1 ë³‘ë ¬ ì²˜ë¦¬ (ë…ë¦½ ì—°ê²° ì‚¬ìš©)"""
        try:
            return self.process_api1_station_passenger(date_str, date_str)
        except Exception as e:
            logger.error(f"API1 parallel processing failed for {date_str}: {e}")
            return 0
    
    def _process_api2_parallel(self, date_str: str) -> int:
        """API2 ë³‘ë ¬ ì²˜ë¦¬ (ë…ë¦½ ì—°ê²° ì‚¬ìš©)"""
        try:
            return self.process_api2_section_passenger(date_str, date_str)
        except Exception as e:
            logger.error(f"API2 parallel processing failed for {date_str}: {e}")
            return 0
    
    def _process_api3_parallel(self, date_str: str) -> int:
        """API3 ë³‘ë ¬ ì²˜ë¦¬ (ë…ë¦½ ì—°ê²° ì‚¬ìš©)"""
        try:
            return self.process_api3_emd_od(date_str, date_str)
        except Exception as e:
            logger.error(f"API3 parallel processing failed for {date_str}: {e}")
            return 0
    
    def _process_api4_parallel(self, date_str: str) -> int:
        """API4 ë³‘ë ¬ ì²˜ë¦¬ (ë…ë¦½ ì—°ê²° ì‚¬ìš©)"""
        try:
            return self.process_api4_section_speed(date_str, date_str)
        except Exception as e:
            logger.error(f"API4 parallel processing failed for {date_str}: {e}")
            return 0

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)"""
    # DB ì„¤ì •
    db_config = {
        'host': os.getenv('DB_HOST', 'localhost'),
        'port': int(os.getenv('DB_PORT', 5432)),
        'database': os.getenv('DB_NAME', 'ddf_db'),
        'user': os.getenv('DB_USER', 'ddf_user'),
        'password': os.getenv('DB_PASSWORD', 'ddf_password')
    }
    
    # ì„±ëŠ¥ ìµœì í™”ëœ ETL í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
    etl = SeoulTrafficETL(db_config)
    
    # ì‹¤í–‰ ëª¨ë“œ ì„ íƒ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´ ê°€ëŠ¥)
    parallel_mode = os.getenv('ETL_PARALLEL_MODE', 'true').lower() == 'true'
    
    if parallel_mode:
        logger.info("ğŸš€ Starting High-Performance Parallel Seoul Traffic ETL Process")
        logger.info("âš¡ Performance Features: Connection Pool + Parallel Processing + Optimized Batching")
        logger.info("ğŸ“Š APIs to process: API1, API2, API3, API4 (Parallel Execution)")
        logger.info("ğŸ“… Processing with 4 parallel workers for maximum performance")
        
        # ë³‘ë ¬ ETL ì‹¤í–‰ (ê¸°ë³¸ ë‚ ì§œ ë²”ìœ„: 2025-07-19 ~ 2025-07-31)
        etl.run_parallel_etl()
        etl_success = True  # ETL ì™„ë£Œ ê°€ì •
    else:
        logger.info("ğŸš€ Starting Standard Seoul Traffic ETL Process")
        logger.info("ğŸ“Š APIs to process: API1, API2, API3, API4 (Sequential Execution)")
        logger.info("ğŸ“… All APIs will process the full date range sequentially")
        
        # ê¸°ë³¸ ETL ì‹¤í–‰ (ìˆœì°¨ ì²˜ë¦¬)
        etl.run_full_etl()
        etl_success = True  # ETL ì™„ë£Œ ê°€ì •
    
    # ETL ì™„ë£Œ í›„ DRT ì§‘ê³„ ì‹¤í–‰ (ê¸°ì¡´ MV ê°±ì‹  ë°©ì‹ê³¼ ë™ì¼)
    logger.info("ğŸ¯ Starting DRT Score Aggregation...")
    try:
        # DB ì—°ê²°ì´ ì—†ìœ¼ë©´ ì—°ê²°
        if not etl.conn or etl.conn.closed:
            etl.connect_db()
        
        # mv_station_hourly_patternsì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì›” ì¡°íšŒ
        check_months_sql = """
            SELECT DISTINCT month_date 
            FROM mv_station_hourly_patterns 
            ORDER BY month_date DESC
        """
        etl.cur.execute(check_months_sql)
        available_months = [row[0] for row in etl.cur.fetchall()]
        
        if available_months:
            logger.info(f"ğŸ“… Found {len(available_months)} months to aggregate: {[m.strftime('%Y-%m') for m in available_months]}")
            
            # ê° ì›”ë³„ë¡œ DRT ì§‘ê³„ í•¨ìˆ˜ ì‹¤í–‰
            for month_date in available_months:
                logger.info(f"ğŸš€ Processing DRT aggregation for {month_date.strftime('%Y-%m')}...")
                
                # 3ê°œ DRT ëª¨ë¸ ëª¨ë‘ ì‹¤í–‰
                for model_name, function_name in [
                    ('Commuter', 'calculate_commuter_drt_scores'),
                    ('Tourism', 'calculate_tourism_drt_scores'), 
                    ('Vulnerable', 'calculate_vulnerable_drt_scores')
                ]:
                    try:
                        logger.info(f"  âš¡ Running {model_name} model...")
                        drt_sql = f"SELECT {function_name}(%s);"
                        etl.cur.execute(drt_sql, (month_date,))
                        result = etl.cur.fetchone()[0]
                        etl.conn.commit()
                        logger.info(f"  âœ… {model_name} completed: {result:,} records")
                    except Exception as e:
                        logger.warning(f"  âš ï¸ {model_name} failed: {e}")
                        # ì‹¤íŒ¨í•´ë„ ë‹¤ìŒ ëª¨ë¸ ê³„ì† ì§„í–‰
                
                logger.info(f"âœ… {month_date.strftime('%Y-%m')} DRT aggregation completed")
            
            logger.info("ğŸ‰ All DRT Score aggregation completed!")
        else:
            logger.warning("âš ï¸ No months found in mv_station_hourly_patterns, skipping DRT aggregation")
            
    except Exception as e:
        logger.warning(f"âš ï¸ DRT Score aggregation failed: {e}")
        # ì‹¤íŒ¨í•´ë„ ì „ì²´ ETLì€ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
        logger.info("   ETL data is still available, DRT aggregation can be run manually")

if __name__ == "__main__":
    main()