#!/usr/bin/env python3
"""
DRT Features ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ CSV íŒŒì¼ ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸
ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
"""

import psycopg2
import pandas as pd
import numpy as np
from datetime import datetime
import os
import sys
from typing import List, Optional
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('drt_extraction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DRTDataExtractor:
    def __init__(self, db_config: dict, output_dir: str = "data/processed"):
        self.db_config = db_config
        self.output_dir = output_dir
        self.essential_columns = [
            'stop_id', 'stop_name', 'recorded_at', 'latitude', 'longitude',
            'normalized_log_boarding_count', 'service_availability', 
            'is_rest_day', 'normalized_interval', 'drt_probability'
        ]
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
    
    def get_data_info(self) -> dict:
        """ë°ì´í„° í¬ê¸° ë° ê¸°ë³¸ ì •ë³´ ì¡°íšŒ (2024-11-01 ~ 2025-06-25)"""
        query = """
        SELECT 
            COUNT(*) as total_records,
            COUNT(DISTINCT df.stop_id) as unique_stops,
            MIN(df.recorded_at) as min_date,
            MAX(df.recorded_at) as max_date,
            AVG(LENGTH(bs.stop_name)) as avg_name_length
        FROM drt_features_mstgcn df
        JOIN bus_stops bs ON df.stop_id = bs.stop_id
        WHERE bs.latitude IS NOT NULL 
        AND bs.longitude IS NOT NULL
        AND df.recorded_at >= '2024-11-01 00:00:00'
        AND df.recorded_at <= '2025-06-25 23:59:59'
        """
        
        try:
            with psycopg2.connect(**self.db_config) as conn:
                df = pd.read_sql(query, conn)
                info = df.iloc[0].to_dict()
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (MB)
                estimated_memory = (info['total_records'] * len(self.essential_columns) * 20) / (1024**2)
                info['estimated_memory_mb'] = estimated_memory
                
                return info
        except Exception as e:
            logger.error(f"ë°ì´í„° ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            raise
    
    def extract_batch(self, offset: int, batch_size: int) -> pd.DataFrame:
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì¶”ì¶œ (2024-11-01 ~ 2025-06-25)"""
        query = """
        SELECT 
            df.stop_id,
            bs.stop_name,
            df.recorded_at,
            bs.latitude,
            bs.longitude,
            df.normalized_log_boarding_count,
            df.service_availability,
            df.is_rest_day,
            df.normalized_interval,
            df.drt_probability
        FROM drt_features_mstgcn df
        JOIN bus_stops bs ON df.stop_id = bs.stop_id
        WHERE bs.latitude IS NOT NULL 
        AND bs.longitude IS NOT NULL
        AND df.recorded_at >= '2024-11-01 00:00:00'
        AND df.recorded_at <= '2025-06-25 23:59:59'
        ORDER BY df.recorded_at, df.stop_id
        LIMIT %s OFFSET %s
        """
        
        try:
            with psycopg2.connect(**self.db_config) as conn:
                df = pd.read_sql(query, conn, params=[batch_size, offset])
                return df
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì¶”ì¶œ ì‹¤íŒ¨ (offset: {offset}, batch_size: {batch_size}): {e}")
            raise
    
    def extract_to_csv(self, batch_size: int = 50000, max_memory_mb: int = 500) -> str:
        """ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë°°ì¹˜ ì²˜ë¦¬ë¡œ CSV íŒŒì¼ ìƒì„±"""
        
        logger.info("=== DRT Features CSV ì¶”ì¶œ ì‹œì‘ ===")
        
        # 1. ë°ì´í„° ì •ë³´ í™•ì¸
        info = self.get_data_info()
        total_records = info['total_records']
        unique_stops = info['unique_stops']
        estimated_memory = info['estimated_memory_mb']
        
        logger.info(f"ì´ ë ˆì½”ë“œ ìˆ˜: {total_records:,}")
        logger.info(f"ìœ ë‹ˆí¬ ì •ë¥˜ì¥: {unique_stops:,}")
        logger.info(f"ë°ì´í„° ê¸°ê°„: {info['min_date']} ~ {info['max_date']}")
        logger.info(f"ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {estimated_memory:.1f} MB")
        
        # 2. ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if estimated_memory > max_memory_mb:
            adjusted_batch_size = min(batch_size, int(batch_size * max_memory_mb / estimated_memory))
            logger.warning(f"ë©”ëª¨ë¦¬ ì œí•œìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ì¡°ì •: {batch_size} â†’ {adjusted_batch_size}")
            batch_size = adjusted_batch_size
        
        # 3. ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(self.output_dir, f"drt_features_{timestamp}.csv")
        
        # 4. ë°°ì¹˜ë³„ ì²˜ë¦¬
        total_batches = (total_records + batch_size - 1) // batch_size
        processed_records = 0
        
        logger.info(f"ë°°ì¹˜ í¬ê¸°: {batch_size:,}")
        logger.info(f"ì´ ë°°ì¹˜ ìˆ˜: {total_batches}")
        logger.info(f"ì¶œë ¥ íŒŒì¼: {output_file}")
        
        # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ í—¤ë” ìƒì„±
        first_batch = True
        
        for batch_num in range(total_batches):
            offset = batch_num * batch_size
            
            logger.info(f"ë°°ì¹˜ {batch_num + 1}/{total_batches} ì²˜ë¦¬ ì¤‘... (offset: {offset:,})")
            
            try:
                # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
                batch_df = self.extract_batch(offset, batch_size)
                
                if batch_df.empty:
                    logger.warning(f"ë°°ì¹˜ {batch_num + 1}ì´ ë¹„ì–´ìˆìŒ")
                    break
                
                # ğŸ”„ ì‹œê°„ìˆœ ì •ë ¬ ì¶”ê°€ (recorded_at, stop_id ê¸°ì¤€)
                batch_df = batch_df.sort_values(by=['recorded_at', 'stop_id'])
                logger.info(f"  â†’ ë°°ì¹˜ ì •ë ¬ ì™„ë£Œ: {len(batch_df):,}ê°œ ë ˆì½”ë“œ")
                
                # CSV íŒŒì¼ì— ì¶”ê°€
                mode = 'w' if first_batch else 'a'
                header = first_batch
                
                batch_df.to_csv(output_file, mode=mode, header=header, index=False)
                
                processed_records += len(batch_df)
                first_batch = False
                
                logger.info(f"  â†’ {len(batch_df):,}ê°œ ë ˆì½”ë“œ ì²˜ë¦¬ ì™„ë£Œ (ëˆ„ì : {processed_records:,})")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del batch_df
                
            except Exception as e:
                logger.error(f"ë°°ì¹˜ {batch_num + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                raise
        
        # 5. ìµœì¢… ê²€ì¦
        logger.info("=== ì¶”ì¶œ ì™„ë£Œ ===")
        logger.info(f"ì´ ì²˜ë¦¬ëœ ë ˆì½”ë“œ: {processed_records:,}")
        logger.info(f"ì¶œë ¥ íŒŒì¼: {output_file}")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size_mb = os.path.getsize(output_file) / (1024**2)
        logger.info(f"íŒŒì¼ í¬ê¸°: {file_size_mb:.1f} MB")
        
        return output_file
    
    def extract_sample(self, sample_size: int = 10000) -> str:
        """ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ (í…ŒìŠ¤íŠ¸ìš©)"""
        logger.info(f"=== ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ (í¬ê¸°: {sample_size:,}) ===")
        
        query = """
        SELECT 
            df.stop_id,
            bs.stop_name,
            df.recorded_at,
            bs.latitude,
            bs.longitude,
            df.normalized_log_boarding_count,
            df.service_availability,
            df.is_rest_day,
            df.normalized_interval,
            df.drt_probability
        FROM drt_features_mstgcn df
        JOIN bus_stops bs ON df.stop_id = bs.stop_id
        WHERE bs.latitude IS NOT NULL 
        AND bs.longitude IS NOT NULL
        ORDER BY RANDOM()
        LIMIT %s
        """
        
        try:
            with psycopg2.connect(**self.db_config) as conn:
                df = pd.read_sql(query, conn, params=[sample_size])
            
            # ì¶œë ¥ íŒŒì¼
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = os.path.join(self.output_dir, f"drt_features_sample_{timestamp}.csv")
            
            df.to_csv(output_file, index=False)
            
            logger.info(f"ìƒ˜í”Œ íŒŒì¼ ìƒì„±: {output_file}")
            logger.info(f"ë ˆì½”ë“œ ìˆ˜: {len(df):,}")
            logger.info(f"íŒŒì¼ í¬ê¸°: {os.path.getsize(output_file) / 1024:.1f} KB")
            
            return output_file
            
        except Exception as e:
            logger.error(f"ìƒ˜í”Œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # DB ì—°ê²° ì„¤ì •
    db_config = {
        'host': 'localhost',
        'port': 5432,
        'database': 'ddf_db',
        'user': 'ddf_user',
        'password': 'ddf_password'
    }
    
    # ì¶”ì¶œê¸° ìƒì„±
    extractor = DRTDataExtractor(db_config)
    
    try:
        # ëª…ë ¹í–‰ ì¸ì í™•ì¸
        if len(sys.argv) > 1 and sys.argv[1] == 'sample':
            # ìƒ˜í”Œ ì¶”ì¶œ
            sample_size = int(sys.argv[2]) if len(sys.argv) > 2 else 10000
            output_file = extractor.extract_sample(sample_size)
        else:
            # ì „ì²´ ë°ì´í„° ì¶”ì¶œ
            batch_size = int(sys.argv[1]) if len(sys.argv) > 1 else 50000
            output_file = extractor.extract_to_csv(batch_size)
        
        print(f"\nâœ… ì¶”ì¶œ ì™„ë£Œ!")
        print(f"ì¶œë ¥ íŒŒì¼: {output_file}")
        
        # ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥
        df_sample = pd.read_csv(output_file, nrows=1000)
        print(f"\nğŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
        print(df_sample.head())
        print(f"\nğŸ“‹ ë°ì´í„° ì •ë³´:")
        print(df_sample.info())
        
    except Exception as e:
        logger.error(f"ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()